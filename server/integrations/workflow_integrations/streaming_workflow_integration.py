#!/usr/bin/env python3
"""
StreamingWorkflowIntegration - —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º: —Ç–µ–∫—Å—Ç ‚Üí –∞—É–¥–∏–æ ‚Üí –∫–ª–∏–µ–Ω—Ç
"""

import logging
import asyncio
from typing import Dict, Any, AsyncGenerator, Optional, Union, Set
from datetime import datetime
from dataclasses import dataclass, field

from config.unified_config import WorkflowConfig, get_config
from integrations.core.assistant_response_parser import AssistantResponseParser
from utils.logging_formatter import log_structured

logger = logging.getLogger(__name__)


@dataclass
class RequestContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    session_id: str
    stream_buffer: str = ""
    pending_segment: str = ""
    processed_sentences: Set[int] = field(default_factory=set)
    json_buffer: str = ""
    pending_command_payload: Optional[Dict[str, Any]] = None
    command_payload_sent: bool = False
    json_parsed: bool = False
    has_emitted: bool = False


class StreamingWorkflowIntegration:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ‚Üí –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ ‚Üí —Å—Ç—Ä–∏–º–∏–Ω–≥ –∫–ª–∏–µ–Ω—Ç—É
    """
    
    def __init__(
        self,
        text_processor=None,
        audio_processor=None,
        memory_workflow=None,
        text_filter_manager=None,
        workflow_config: Optional[Union[WorkflowConfig, Dict[str, Any]]] = None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StreamingWorkflowIntegration
        
        Args:
            text_processor: –ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (UniversalModuleInterface)
            audio_processor: –ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ (UniversalModuleInterface)
            memory_workflow: Workflow –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é
            text_filter_manager: –ú–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (UniversalModuleInterface)
        """
        # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ (–Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.text_module = text_processor
        self.audio_module = audio_processor
        self.memory_workflow = memory_workflow
        self.text_filter_module = text_filter_manager
        self.is_initialized = False
        
        # –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Ç–µ–ø–µ—Ä—å –≤ RequestContext (request-scoped), –Ω–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        # –£–¥–∞–ª–µ–Ω—ã: self._stream_buffer, self._has_emitted, self._pending_segment, 
        #          self._processed_sentences, self._pending_command_payload, 
        #          self._command_payload_sent, self._json_buffer, self._json_parsed
        
        self._assistant_parser = AssistantResponseParser()
        
        # –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        if workflow_config is None:
            workflow_config = get_config().get_workflow_thresholds()

        if isinstance(workflow_config, WorkflowConfig):
            cfg = workflow_config
        else:
            cfg = WorkflowConfig(**workflow_config)

        self.stream_min_chars: int = cfg.stream_min_chars
        self.stream_min_words: int = cfg.stream_min_words
        self.stream_first_sentence_min_words: int = cfg.stream_first_sentence_min_words
        self.stream_punct_flush_strict: bool = bool(cfg.stream_punct_flush_strict)
        self.force_flush_max_chars: int = cfg.force_flush_max_chars
        self.sentence_joiner: str = " "
        self.end_punctuations = ('.', '!', '?')
        
        # Single-flight –∑–∞—â–∏—Ç–∞ –ø–æ session_id (atomic in-flight set)
        self._inflight_sessions: set[str] = set()
        self._inflight_lock = asyncio.Lock()
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        logger.info(
            f"üîß StreamingWorkflowIntegration —Å–æ–∑–¥–∞–Ω: instance_id={id(self)}, inflight_set_id={id(self._inflight_sessions)}",
            extra={
                'scope': 'workflow',
                'method': '__init__',
                'instance_id': id(self),
                'inflight_set_id': id(self._inflight_sessions)
            }
        )
    
    async def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StreamingWorkflowIntegration...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥—É–ª–µ–π
            if not self.text_module:
                logger.warning("‚ö†Ô∏è TextProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.audio_module:
                logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.memory_workflow:
                logger.warning("‚ö†Ô∏è MemoryWorkflow –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.text_filter_module:
                logger.warning("‚ö†Ô∏è TextFilterManager –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            self.is_initialized = True
            logger.info("‚úÖ StreamingWorkflowIntegration –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ StreamingWorkflowIntegration: {e}")
            return False
    
    async def process_request_streaming(self, request_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º—è—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
        if not self.is_initialized:
            logger.error("‚ùå StreamingWorkflowIntegration –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            yield {
                'success': False,
                'error': 'StreamingWorkflowIntegration not initialized',
                'text_response': '',
            }
            return

        session_id = request_data.get('session_id')
        if not session_id or session_id == 'unknown':
            # –ö–†–ò–¢–ò–ß–ù–û: session_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –≤ grpc_server.py
            logger.error(
                f"‚ùå session_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ä–∞–≤–µ–Ω 'unknown' - –Ω–∞—Ä—É—à–µ–Ω–∏–µ Source of Truth",
                extra={
                    'scope': 'workflow',
                    'method': 'process_request_streaming',
                    'decision': 'error',
                    'ctx': {'session_id': session_id, 'reason': 'missing_session_id'}
                }
            )
            yield {
                'success': False,
                'error': 'session_id must be provided by gRPC layer',
                'error_code': 'INVALID_ARGUMENT',
                'error_type': 'missing_session_id',
                'text_response': '',
            }
            return

        # –°–û–ó–î–ê–ï–ú request-scoped –∫–æ–Ω—Ç–µ–∫—Å—Ç
        ctx = RequestContext(session_id=session_id)
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ single-flight –ø—Ä–æ–≤–µ—Ä–∫–æ–π
        logger.info(
            f"üîç Single-flight check: session_id={session_id}, instance_id={id(self)}, "
            f"inflight_set_id={id(self._inflight_sessions)}, current_inflight={list(self._inflight_sessions)}",
            extra={
                'scope': 'workflow',
                'method': 'process_request_streaming',
                'session_id': session_id,
                'instance_id': id(self),
                'inflight_set_id': id(self._inflight_sessions),
                'current_inflight_count': len(self._inflight_sessions)
            }
        )
        
        # Atomic single-flight: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥ –æ–¥–Ω–∏–º lock
        async with self._inflight_lock:
            if session_id in self._inflight_sessions:
                # –£–∂–µ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —ç—Ç–∏–º session_id
                logger.warning(
                    f"‚ö†Ô∏è –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å session_id={session_id} –æ—Ç–∫–ª–æ–Ω—ë–Ω (single-flight) - "
                    f"instance_id={id(self)}, inflight_set_id={id(self._inflight_sessions)}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'decision': 'reject',
                        'ctx': {'session_id': session_id, 'reason': 'concurrent_request'},
                        'instance_id': id(self),
                        'inflight_set_id': id(self._inflight_sessions)
                    }
                )
                yield {
                    'success': False,
                    'error': f'Concurrent request for session_id={session_id} is not allowed',
                    'error_code': 'RESOURCE_EXHAUSTED',
                    'error_type': 'concurrent_request',
                    'text_response': '',
                }
                return
            
            # –î–æ–±–∞–≤–ª—è–µ–º session_id –≤ in-flight set
            self._inflight_sessions.add(session_id)
            logger.info(
                f"‚úÖ Session –¥–æ–±–∞–≤–ª–µ–Ω –≤ inflight: session_id={session_id}, instance_id={id(self)}, "
                f"inflight_set_id={id(self._inflight_sessions)}, new_inflight={list(self._inflight_sessions)}",
                extra={
                    'scope': 'workflow',
                    'method': 'process_request_streaming',
                    'session_id': session_id,
                    'instance_id': id(self),
                    'inflight_set_id': id(self._inflight_sessions),
                    'action': 'added_to_inflight'
                }
            )
        
        try:
            import time
            request_start_time = time.time()
            
            logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {session_id}")
            logger.info(f"‚Üí Input text len={len(request_data.get('text','') or '')}, has_screenshot={bool(request_data.get('screenshot'))}")
            logger.info(f"‚Üí Input text content: '{request_data.get('text', '')[:100]}...'")

            logger.info("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–£–õ–ï–ô:")
            logger.info(f"   ‚Üí text_processor: {self.text_module is not None}")
            logger.info(f"   ‚Üí audio_processor: {self.audio_module is not None}")
            if self.text_module:
                logger.info(f"   ‚Üí text_processor.is_initialized: {getattr(self.text_module, 'is_initialized', 'NO_ATTR')}")
            if self.audio_module:
                logger.info(f"   ‚Üí audio_processor.is_initialized: {getattr(self.audio_module, 'is_initialized', 'NO_ATTR')}")

            hardware_id = request_data.get('hardware_id', 'unknown')
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ hardware_id
            if hardware_id != 'unknown' and self.memory_workflow:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –≤ —Ñ–æ–Ω–µ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É)
                asyncio.create_task(
                    self.memory_workflow.prefetch_memory(hardware_id)
                )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞–º—è—Ç—å (–∏–∑ –∫—ç—à–∞ –∏–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º)
            memory_start_time = time.time()
            memory_context = await self._get_memory_context_parallel(hardware_id)
            memory_time = (time.time() - memory_start_time) * 1000
            memory_size = len(str(memory_context)) if memory_context else 0
            logger.info(f"‚è±Ô∏è  Memory context –ø–æ–ª—É—á–µ–Ω –∑–∞ {memory_time:.2f}ms (—Ä–∞–∑–º–µ—Ä: {memory_size} —Å–∏–º–≤–æ–ª–æ–≤)")
            MAX_JSON_BUFFER_SIZE = 10000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ (10KB)
            json_parse_attempts = 0  # –°—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
            MAX_JSON_PARSE_ATTEMPTS = 10  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON

            captured_segments: list[str] = []
            input_sentence_counter = 0
            emitted_segment_counter = 0
            total_audio_chunks = 0
            total_audio_bytes = 0
            sentence_audio_map: dict[int, int] = {}
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
            first_text_time = None
            first_audio_time = None
            llm_start_time = time.time()

            async for sentence in self._iter_processed_sentences(
                request_data.get('text', ''),
                request_data.get('screenshot'),
                memory_context
            ):
                if first_text_time is None:
                    first_text_time = (time.time() - llm_start_time) * 1000
                    logger.info(f"‚è±Ô∏è  –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {first_text_time:.2f}ms")
                input_sentence_counter += 1
                logger.debug(f"üìù In sentence #{input_sentence_counter}: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
                if len(ctx.json_buffer) + len(sentence) > MAX_JSON_BUFFER_SIZE:
                    logger.warning(f"‚ö†Ô∏è JSON –±—É—Ñ–µ—Ä –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç ({MAX_JSON_BUFFER_SIZE} —Å–∏–º–≤–æ–ª–æ–≤), —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                    if ctx.json_buffer:
                        parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                        sentence = parsed.text_response
                    else:
                        # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –ø—É—Å—Ç, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                        parsed = await self._parse_assistant_response(sentence, session_id)
                        sentence = parsed.text_response
                    ctx.json_buffer = ""
                    json_parse_attempts = 0
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º JSON –±–ª–æ–∫)
                else:
                    # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON: –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å –≤ –±—É—Ñ–µ—Ä
                    ctx.json_buffer += sentence
                    
                    # –û—á–∏—â–∞–µ–º –æ—Ç markdown –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                    cleaned_buffer = self._extract_json_from_markdown(ctx.json_buffer)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –±—É—Ñ–µ—Ä —Å JSON (–º–æ–∂–µ—Ç –±—ã—Ç—å `{` –∏–ª–∏ markdown-–±–ª–æ–∫)
                    is_potential_json = cleaned_buffer.strip().startswith('{')
                    
                    if is_potential_json:
                        json_parse_attempts += 1
                        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π JSON (–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è markdown-—Ä–∞–∑–º–µ—Ç–∫–∏)
                        try:
                            import json
                            parsed_json: Dict[str, Any] = json.loads(cleaned_buffer)
                            # JSON –≤–∞–ª–∏–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                            logger.info(f"‚úÖ JSON –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞–∫–æ–ø–ª–µ–Ω –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_buffer)})")
                            ctx.json_parsed = True
                            json_parse_attempts = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ
                            
                            # JSON –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞–∫–æ–ø–ª–µ–Ω - –ø–∞—Ä—Å–∏–º –µ–≥–æ
                            parsed = await self._parse_assistant_response(parsed_json, session_id)
                            # –û—á–∏—â–∞–µ–º JSON –±—É—Ñ–µ—Ä –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
                            ctx.json_buffer = ""
                            ctx.json_parsed = False
                        except (json.JSONDecodeError, ValueError):
                            # JSON –µ—â—ë –Ω–µ –ø–æ–ª–Ω—ã–π - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å
                            if json_parse_attempts >= MAX_JSON_PARSE_ATTEMPTS:
                                # –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
                                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON ({MAX_JSON_PARSE_ATTEMPTS}), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                                parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                                sentence = parsed.text_response
                                ctx.json_buffer = ""
                                json_parse_attempts = 0
                                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º JSON –±–ª–æ–∫)
                            else:
                                logger.debug(f"üì¶ –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {json_parse_attempts}/{MAX_JSON_PARSE_ATTEMPTS})")
                                continue
                    else:
                        # –≠—Ç–æ –Ω–µ JSON - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–µ–¥–∞—ë–º —á–∞—Å—Ç—è–º–∏)
                        logger.debug(f"üìù –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–Ω–µ JSON): {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤, –ø–µ—Ä–µ–¥–∞—ë–º —á–∞—Å—Ç—è–º–∏")
                        # –û—á–∏—â–∞–µ–º JSON –±—É—Ñ–µ—Ä, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–µ JSON
                        ctx.json_buffer = ""
                        json_parse_attempts = 0
                        # –ü–∞—Ä—Å–∏–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å —Ñ–æ—Ä–º–∞—Ç {"text": "..."} –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç)
                        parsed = await self._parse_assistant_response(sentence, session_id)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ parsed (–¥–ª—è –æ–±–æ–∏—Ö —Å–ª—É—á–∞–µ–≤: JSON –∏ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç)
                if parsed.command_payload and not ctx.command_payload_sent:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º command_payload –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑
                    ctx.pending_command_payload = parsed.command_payload
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
                    self._log_command_detected(parsed, session_id)

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ text_response –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                sentence = parsed.text_response

                # [–ù–û–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï] –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã
                if parsed.command_payload and sentence and sentence.strip():
                    logger.debug(f"üé§ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –∫–æ–º–∞–Ω–¥—ã: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                    emitted_segment_counter += 1
                    captured_segments.append(sentence.strip())
                    
                    # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –∏ –∞—É–¥–∏–æ, –º–∏–Ω—É—è –±—É—Ñ–µ—Ä
                    yield { 'success': True, 'text_response': sentence.strip(), 'sentence_index': emitted_segment_counter }
                    
                    tts_text = sentence.strip() if sentence.strip().endswith(self.end_punctuations) else f"{sentence.strip()}."
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å—Ç—Ä–∏–º–∏–º –∞—É–¥–∏–æ —á–∞–Ω–∫–∏
                    segment_audio_chunks = 0
                    async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                        if audio_chunk:
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency
                            segment_audio_chunks += 1
                            total_audio_chunks += 1
                            total_audio_bytes += len(audio_chunk)
                            yield { 
                                'success': True, 
                                'audio_chunk': audio_chunk,
                                'sentence_index': emitted_segment_counter 
                            }
                    
                    sentence_audio_map[emitted_segment_counter] = segment_audio_chunks
                    logger.debug(f"üéß Command confirmation audio generated for segment #{emitted_segment_counter}: {segment_audio_chunks} —á–∞–Ω–∫–æ–≤, {total_audio_bytes} –±–∞–π—Ç")

                    # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å —Ü–∏–∫–ª–∞, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ—Ç —á–∞–Ω–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
                    ctx.json_buffer = ""
                    ctx.json_parsed = False
                    continue

                # –ï–¥–∏–Ω–∞—è –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è: –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º, –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                # –í–ê–ñ–ù–û: –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ, text_response –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è TTS
                if not sentence or not sentence.strip():
                    logger.warning(f"‚ö†Ô∏è text_response –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É TTS")
                    continue
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ sentence –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ JSON, –ø—Ä–æ–±—É–µ–º –µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
                # –≠—Ç–æ –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ JSON –Ω–µ –±—ã–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω —Ä–∞–Ω–µ–µ –∏ –ø–æ–ø–∞–ª –≤ stream_buffer
                if sentence.strip().startswith('{') or '{"text"' in sentence or '"text":' in sentence:
                    # –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ JSON, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –±—ã–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω - –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
                    try:
                        import json
                        cleaned = self._extract_json_from_markdown(sentence)
                        if cleaned.strip().startswith('{'):
                            parsed_json = json.loads(cleaned)
                            parsed = await self._parse_assistant_response(parsed_json, session_id)
                            sentence = parsed.text_response
                            logger.debug(f"‚úÖ JSON —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ TTS: {len(sentence) if sentence else 0} —Å–∏–º–≤–æ–ª–æ–≤")
                    except (json.JSONDecodeError, ValueError):
                        # –ù–µ JSON –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–π - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        pass
                
                logger.debug(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ text_response –¥–ª—è TTS: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                sanitized = await self._sanitize_for_tts(sentence)
                if sanitized:
                    # –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º sanitized_hash –≤ ctx.processed_sentences –∑–¥–µ—Å—å,
                    # —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –ø—Ä–æ–ø—É—Å–∫—É —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
                    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —ç—Ç–∞–ø–µ —ç–º–∏—Å—Å–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤.
                    ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{sanitized}" if ctx.stream_buffer else sanitized)
                    logger.debug(f"üì¶ stream_buffer –æ–±–Ω–æ–≤–ª–µ–Ω: {len(ctx.stream_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                else:
                    logger.warning(f"‚ö†Ô∏è sanitized –ø—É—Å—Ç–æ–π –¥–ª—è sentence: '{sentence[:50]}...'")

                logger.debug(f"üîç –í—ã–∑–æ–≤ _split_complete_sentences —Å stream_buffer: {len(ctx.stream_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                complete_sentences, remainder = await self._split_complete_sentences(ctx.stream_buffer)
                logger.debug(f"‚úÖ _split_complete_sentences –≤–µ—Ä–Ω—É–ª: {len(complete_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, remainder={len(remainder) if remainder else 0} —Å–∏–º–≤–æ–ª–æ–≤")
                ctx.stream_buffer = remainder

                for complete in complete_sentences:
                    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ –ø–æ—Ä–æ–≥–æ–≤
                    candidate = complete if not ctx.pending_segment else f"{ctx.pending_segment}{self.sentence_joiner}{complete}"
                    words_count = await self._count_meaningful_words(candidate)
                    logger.debug(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–º–∏—Å—Å–∏–∏: {len(candidate)} —Å–∏–º–≤–æ–ª–æ–≤, {words_count} —Å–ª–æ–≤, has_emitted={ctx.has_emitted}")
                    should_emit = (not ctx.has_emitted and (words_count >= self.stream_first_sentence_min_words or len(candidate) >= self.stream_min_chars)) or \
                       (ctx.has_emitted and (words_count >= self.stream_min_words or len(candidate) >= self.stream_min_chars))
                    logger.debug(f"   ‚Üí should_emit={should_emit}")
                    if should_emit:
                        logger.debug(f"‚úÖ –í–•–û–î –í –ë–õ–û–ö –≠–ú–ò–°–°–ò–ò: {len(candidate)} —Å–∏–º–≤–æ–ª–æ–≤")
                        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π)
                        to_emit = candidate.strip()
                        logger.debug(f"   ‚Üí to_emit: {len(to_emit)} —Å–∏–º–≤–æ–ª–æ–≤")
                        if len(to_emit) > 10:  # –¢–æ–ª—å–∫–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                            complete_hash = hash(to_emit)
                            if complete_hash in ctx.processed_sentences:
                                logger.warning(f"üîÑ –ü–†–û–ü–£–°–ö–ê–ï–ú –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: '{to_emit[:50]}...' (hash={complete_hash})")
                                continue
                            logger.info(f"   ‚Üí –î–æ–±–∞–≤–ª—è–µ–º hash –≤ processed_sentences: {complete_hash}")
                            ctx.processed_sentences.add(complete_hash)
                        
                        # –ì–æ—Ç–æ–≤ –∫ —ç–º–∏—Å—Å–∏–∏
                        logger.info(f"üéØ –ì–û–¢–û–í –ö –≠–ú–ò–°–°–ò–ò: emitted_segment_counter –±—É–¥–µ—Ç {emitted_segment_counter + 1}")
                        emitted_segment_counter += 1
                        ctx.pending_segment = ""
                        ctx.has_emitted = True

                        # –¢–µ–∫—Å—Ç
                        logger.debug(f"üì§ –≠–ú–ò–°–°–ò–Ø –¢–ï–ö–°–¢–ê: {len(to_emit)} —Å–∏–º–≤–æ–ª–æ–≤ (sentence_index={emitted_segment_counter})")
                        captured_segments.append(to_emit)
                        yield {
                            'success': True,
                            'text_response': to_emit,
                            'sentence_index': emitted_segment_counter
                        }

                        # –ê—É–¥–∏–æ (–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –¥–ª—è TTS)
                        # –§–∞–∑–∞ 2: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ text –ø—É—Å—Ç–æ–π
                        if to_emit.strip():
                            tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å—Ç—Ä–∏–º–∏–º –∞—É–¥–∏–æ —á–∞–Ω–∫–∏
                            segment_audio_chunks = 0
                            tts_start_time = time.time()
                            if first_audio_time is None:
                                first_audio_time = (time.time() - request_start_time) * 1000
                                logger.info(f"‚è±Ô∏è  –ü–µ—Ä–≤—ã–π audio_chunk –Ω–∞—á–∞–ª –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ {first_audio_time:.2f}ms")
                            async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                                if not audio_chunk:
                                    continue
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency
                                segment_audio_chunks += 1
                                total_audio_chunks += 1
                                total_audio_bytes += len(audio_chunk)
                                yield {
                                    'success': True,
                                    'audio_chunk': audio_chunk,
                                    'sentence_index': emitted_segment_counter
                                }
                            sentence_audio_map[emitted_segment_counter] = segment_audio_chunks
                            tts_time = (time.time() - tts_start_time) * 1000
                            logger.info(f"‚è±Ô∏è  TTS –¥–ª—è segment #{emitted_segment_counter} –∑–∞–Ω—è–ª {tts_time:.2f}ms ({segment_audio_chunks} —á–∞–Ω–∫–æ–≤, {total_audio_bytes} –±–∞–π—Ç)")
                        else:
                            # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ
                            logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞—É–¥–∏–æ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ segment #{emitted_segment_counter}")
                    else:
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫–æ–ø–∏—Ç—å
                        ctx.pending_segment = candidate

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–ª–∞—à: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è JSON –±—É—Ñ–µ—Ä, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if ctx.json_buffer and not ctx.json_parsed:
                import json
                # –û—á–∏—â–∞–µ–º –æ—Ç markdown –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                cleaned_buffer = self._extract_json_from_markdown(ctx.json_buffer)
                is_potential_json = cleaned_buffer.strip().startswith('{')
                if is_potential_json:
                    try:
                        parsed_json = json.loads(cleaned_buffer)
                        logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –±—É—Ñ–µ—Ä–∞: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_buffer)})")
                        parsed = await self._parse_assistant_response(parsed_json, session_id)
                        if parsed.command_payload and not ctx.command_payload_sent:
                            ctx.pending_command_payload = parsed.command_payload
                            self._log_command_detected(parsed, session_id)
                        # –î–æ–±–∞–≤–ª—è–µ–º text_response –≤ stream_buffer –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        if parsed.text_response:
                            ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{parsed.text_response}" if ctx.stream_buffer else parsed.text_response)
                        ctx.json_buffer = ""
                        ctx.json_parsed = False
                    except (json.JSONDecodeError, ValueError):
                        # JSON –Ω–µ –≤–∞–ª–∏–¥–µ–Ω - –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                        logger.debug(f"‚ö†Ô∏è JSON –±—É—Ñ–µ—Ä –Ω–µ –≤–∞–ª–∏–¥–µ–Ω, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                        if ctx.json_buffer.strip():
                            # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ JSON - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                            parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                            if parsed.text_response:
                                ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{parsed.text_response}" if ctx.stream_buffer else parsed.text_response)
                        ctx.json_buffer = ""
                else:
                    # –≠—Ç–æ –Ω–µ JSON - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                    logger.debug(f"üìù –§–∏–Ω–∞–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä - –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                    if ctx.json_buffer.strip():
                        parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                        if parsed.text_response:
                            ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{parsed.text_response}" if ctx.stream_buffer else parsed.text_response)
                    ctx.json_buffer = ""
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–ª–∞—à: —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞
            # –í–ê–ñ–ù–û: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ stream_buffer JSON-–æ–±—ä–µ–∫—Ç–æ–º
            if ctx.stream_buffer:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ stream_buffer JSON-–æ–±—ä–µ–∫—Ç–æ–º
                stream_cleaned = self._extract_json_from_markdown(ctx.stream_buffer)
                if stream_cleaned.strip().startswith('{'):
                    try:
                        import json
                        parsed_json = json.loads(stream_cleaned)
                        logger.info(f"‚úÖ JSON –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ stream_buffer –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Ñ–ª–∞—à–µ: {len(ctx.stream_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                        parsed = await self._parse_assistant_response(parsed_json, session_id)
                        if parsed.text_response:
                            ctx.stream_buffer = parsed.text_response
                            logger.info(f"üìù –ó–∞–º–µ–Ω—ë–Ω stream_buffer –Ω–∞ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π text_response: '{ctx.stream_buffer[:100]}...' (len={len(ctx.stream_buffer)})")
                    except (json.JSONDecodeError, ValueError):
                        # –ù–µ JSON –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–π - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        pass
                
                complete_sentences, remainder = await self._split_complete_sentences(ctx.stream_buffer)
                ctx.stream_buffer = remainder
                for complete in complete_sentences:
                    candidate = complete if not ctx.pending_segment else f"{ctx.pending_segment}{self.sentence_joiner}{complete}"
                    words_count = await self._count_meaningful_words(candidate)
                    # –ï—Å–ª–∏ –µ—Å—Ç—å command_payload, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —ç–º–∏—Ç–∏—Ä—É–µ–º –¥–∞–∂–µ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                    has_command = ctx.pending_command_payload and not ctx.command_payload_sent
                    should_emit = (
                        (not ctx.has_emitted and (words_count >= self.stream_first_sentence_min_words or len(candidate) >= self.stream_min_chars)) or
                        (ctx.has_emitted and (words_count >= self.stream_min_words or len(candidate) >= self.stream_min_chars)) or
                        (has_command and candidate.strip())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —ç–º–∏—Å—Å–∏—è –¥–ª—è –∫–æ–º–∞–Ω–¥
                    )
                    
                    if should_emit:
                        emitted_segment_counter += 1
                        to_emit = candidate.strip()
                        ctx.pending_segment = ""
                        ctx.has_emitted = True
                        captured_segments.append(to_emit)
                        yield {'success': True, 'text_response': to_emit, 'sentence_index': emitted_segment_counter}
                        # –§–∞–∑–∞ 2: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ text –ø—É—Å—Ç–æ–π
                        if to_emit.strip():
                            tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å—Ç—Ä–∏–º–∏–º –∞—É–¥–∏–æ —á–∞–Ω–∫–∏
                            segment_audio_chunks = 0
                            async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                                if not audio_chunk:
                                    continue
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency
                                total_audio_chunks += 1
                                total_audio_bytes += len(audio_chunk)
                                segment_audio_chunks += 1
                                yield {'success': True, 'audio_chunk': audio_chunk, 'sentence_index': emitted_segment_counter}
                            sentence_audio_map[emitted_segment_counter] = segment_audio_chunks
                            logger.debug(f"üéß Final segment #{emitted_segment_counter} ‚Üí {segment_audio_chunks} —á–∞–Ω–∫–æ–≤, {total_audio_bytes} –±–∞–π—Ç")
                        else:
                            logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞—É–¥–∏–æ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ final segment #{emitted_segment_counter}")
                    else:
                        ctx.pending_segment = candidate
                
                # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è remainder –≤ stream_buffer, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ pending_segment
                if remainder and remainder.strip():
                    if ctx.pending_segment:
                        ctx.pending_segment = f"{ctx.pending_segment}{self.sentence_joiner}{remainder}"
                    else:
                        ctx.pending_segment = remainder

            # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç, –º–æ–∂–Ω–æ —Ñ–æ—Ä—Å-—Ñ–ª–∞—à, –µ—Å–ª–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π
            # –ò–õ–ò –µ—Å–ª–∏ –µ—Å—Ç—å command_payload (–Ω—É–∂–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ–π—Å—Ç–≤–∏—è)
            force_max = self.force_flush_max_chars
            has_command = ctx.pending_command_payload and not ctx.command_payload_sent
            should_force_flush = (
                (force_max > 0 and len(ctx.pending_segment) >= force_max) or
                (has_command and ctx.pending_segment and ctx.pending_segment.strip())
            )
            
            if should_force_flush:
                emitted_segment_counter += 1
                to_emit = ctx.pending_segment
                ctx.pending_segment = ""
                ctx.has_emitted = True
                captured_segments.append(to_emit)
                yield {'success': True, 'text_response': to_emit, 'sentence_index': emitted_segment_counter}
                # –§–∞–∑–∞ 2: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ text –ø—É—Å—Ç–æ–π
                if to_emit.strip():
                    tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                    sentence_audio_chunks = 0
                    async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                        if not audio_chunk:
                            continue
                        sentence_audio_chunks += 1
                        total_audio_chunks += 1
                        total_audio_bytes += len(audio_chunk)
                        yield {'success': True, 'audio_chunk': audio_chunk, 'sentence_index': emitted_segment_counter, 'audio_chunk_index': sentence_audio_chunks}
                    sentence_audio_map[emitted_segment_counter] = sentence_audio_chunks
                    logger.info(f"üéß Forced final segment #{emitted_segment_counter} ‚Üí audio_chunks={sentence_audio_chunks}, total_audio_chunks={total_audio_chunks}, total_bytes={total_audio_bytes}")
                else:
                    logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞—É–¥–∏–æ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ forced segment #{emitted_segment_counter}")

            full_text = " ".join(captured_segments).strip()

            # –§–∞–∑–∞ 2: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º command_payload –æ–¥–∏–Ω —Ä–∞–∑ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ
            final_result = {
                'success': True,
                'text_full_response': full_text,
                'sentences_processed': emitted_segment_counter,
                'audio_chunks_processed': total_audio_chunks,
                'audio_bytes_processed': total_audio_bytes,
                'sentence_audio_map': sentence_audio_map,
                'is_final': True
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º command_payload, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –∏ —Ñ–∏—á–∞-—Ñ–ª–∞–≥ –≤–∫–ª—é—á–µ–Ω
            if ctx.pending_command_payload and not ctx.command_payload_sent:
                config = get_config()
                if (config.features.forward_assistant_actions and 
                    not config.kill_switches.disable_forward_assistant_actions):
                    final_result['command_payload'] = ctx.pending_command_payload
                    ctx.command_payload_sent = True
                    self._log_command_complete(ctx.pending_command_payload, session_id)
                else:
                    logger.debug("–§–∏—á–∞-—Ñ–ª–∞–≥ forward_assistant_actions –≤—ã–∫–ª—é—á–µ–Ω –∏–ª–∏ kill-switch –∞–∫—Ç–∏–≤–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º command_payload")

            total_time = (time.time() - request_start_time) * 1000
            logger.info(
                f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ: segments={emitted_segment_counter}, audio_chunks={total_audio_chunks}, total_bytes={total_audio_bytes}"
            )
            logger.info(f"‚è±Ô∏è  –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –í–†–ï–ú–ï–ù–ò:")
            logger.info(f"   ‚Ä¢ Memory context: {memory_time:.2f}ms")
            if first_text_time:
                logger.info(f"   ‚Ä¢ –î–æ –ø–µ—Ä–≤–æ–≥–æ text (LLM): {first_text_time:.2f}ms")
            if first_audio_time:
                logger.info(f"   ‚Ä¢ –î–æ –ø–µ—Ä–≤–æ–≥–æ audio (TTS): {first_audio_time:.2f}ms")
            logger.info(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}ms ({total_time/1000:.2f} —Å–µ–∫)")
            yield final_result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ {session_id}: {e}")
            yield {
                'success': False,
                'error': str(e),
                'error_code': 'INTERNAL',
                'error_type': 'processing_error',
                'text_response': '',
            }
        finally:
            # –£–¥–∞–ª—è–µ–º session_id –∏–∑ in-flight set (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)
            async with self._inflight_lock:
                was_present = session_id in self._inflight_sessions
                self._inflight_sessions.discard(session_id)
                logger.info(
                    f"üßπ Session —É–¥–∞–ª—ë–Ω –∏–∑ inflight: session_id={session_id}, instance_id={id(self)}, "
                    f"inflight_set_id={id(self._inflight_sessions)}, was_present={was_present}, "
                    f"remaining_inflight={list(self._inflight_sessions)}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'session_id': session_id,
                        'instance_id': id(self),
                        'inflight_set_id': id(self._inflight_sessions),
                        'action': 'removed_from_inflight',
                        'was_present': was_present
                    }
                )

    async def _get_memory_context_parallel(self, hardware_id: str) -> Optional[Dict[str, Any]]:
        """
        –ù–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏
        
        Args:
            hardware_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        """
        try:
            if not self.memory_workflow:
                logger.debug("MemoryWorkflow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
                return None
            
            import time
            start_time = time.time()
            logger.info(f"‚è±Ô∏è  –ù–∞—á–∞–ª–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è {hardware_id}")
            memory_context = await self.memory_workflow.get_memory_context_parallel(hardware_id)
            elapsed = (time.time() - start_time) * 1000
            
            if memory_context:
                context_size = len(str(memory_context))
                logger.info(f"‚è±Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.2f}ms: {len(memory_context)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {context_size} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                logger.info(f"‚è±Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø—É—Å—Ç (–ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.2f}ms)")
            
            return memory_context
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏: {e}")
            return None

    async def _iter_processed_sentences(
        self,
        text: str,
        screenshot: Optional[str],
        memory_context: Optional[Dict[str, Any]]
    ) -> AsyncGenerator[str, None]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞."""
        import time
        enrich_start = time.time()
        enriched_text = self._enrich_with_memory(text, memory_context)
        enrich_time = (time.time() - enrich_start) * 1000
        logger.info(f"‚è±Ô∏è  –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç—å—é –∑–∞–Ω—è–ª–æ {enrich_time:.2f}ms (–∏—Å—Ö–æ–¥–Ω—ã–π: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π: {len(enriched_text)} —Å–∏–º–≤–æ–ª–æ–≤)")

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64 (WebP)
        screenshot_data: Optional[str] = None
        if screenshot:
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64, –ø–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            screenshot_data = screenshot
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä (base64 –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 33% –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
            estimated_size = int(len(screenshot) * 0.75)
            logger.info(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç –ø–æ–ª—É—á–µ–Ω (WebP base64): ~{estimated_size} bytes (base64 –¥–ª–∏–Ω–∞: {len(screenshot)})")

        yielded_any = False
        if self.text_module and hasattr(self.text_module, 'process'):
            llm_start = time.time()
            logger.info(f"‚è±Ô∏è  –ù–∞—á–∞–ª–æ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Text Module: '{enriched_text[:80]}...'")
            try:
                chunk_count = 0
                async for chunk in self._stream_text_module(enriched_text, screenshot_data):
                    chunk_count += 1
                    logger.debug(f"üì¶ –ü–æ–ª—É—á–µ–Ω chunk #{chunk_count} –æ—Ç Text Module: type={type(chunk)}, value={str(chunk)[:100] if chunk else 'None'}...")
                    sentence = (self._extract_text_chunk(chunk) or '').strip()
                    if sentence:
                        if chunk_count == 1:
                            first_chunk_time = (time.time() - llm_start) * 1000
                            logger.info(f"‚è±Ô∏è  –ü–µ—Ä–≤—ã–π chunk –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {first_chunk_time:.2f}ms")
                        yielded_any = True
                        logger.info(f"üì® TextModule sentence #{chunk_count}: '{sentence[:120]}...' (len={len(sentence)})")
                        yield sentence
                    else:
                        logger.warning(f"‚ö†Ô∏è Chunk #{chunk_count} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è")
                llm_total_time = (time.time() - llm_start) * 1000
                logger.info(f"‚è±Ô∏è  LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {llm_total_time:.2f}ms: –ø–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks, yielded_any={yielded_any}")
            except Exception as processing_error:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Text Module: {processing_error}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                import traceback
                traceback.print_exc()
        elif self.text_module and hasattr(self.text_module, 'process_text_streaming'):
            # Legacy fallback –Ω–∞ –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ TextProcessor
            logger.info(f"üîÑ Legacy —Å—Ç—Ä–∏–º–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞: '{enriched_text[:80]}...'")
            try:
                json_buffer = ""  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON –∏–∑ —á–∞–Ω–∫–æ–≤
                json_attempts = 0  # –°—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
                MAX_JSON_BUFFER_SIZE = 10000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ (10KB)
                MAX_JSON_ATTEMPTS = 10  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
                
                async for processed_sentence in self.text_module.process_text_streaming(enriched_text, screenshot_data):
                    # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ processed_sentence - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –∞ –Ω–µ —Ñ—É–Ω–∫—Ü–∏—è
                    if callable(processed_sentence):
                        logger.warning("‚ö†Ô∏è processed_sentence is callable, skipping")
                        continue
                    
                    sentence = (processed_sentence or '').strip()
                    if not sentence:
                        continue
                    
                    # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
                    if len(json_buffer) + len(sentence) > MAX_JSON_BUFFER_SIZE:
                        logger.warning(f"‚ö†Ô∏è JSON –±—É—Ñ–µ—Ä –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç ({MAX_JSON_BUFFER_SIZE} —Å–∏–º–≤–æ–ª–æ–≤), —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                        yielded_any = True
                        yield json_buffer + sentence
                        json_buffer = ""
                        json_attempts = 0
                        continue
                    
                    # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON –∏–∑ —á–∞–Ω–∫–æ–≤
                    json_buffer += sentence
                    cleaned_buffer = self._extract_json_from_markdown(json_buffer)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –ø–æ–ª–Ω—ã–º JSON
                    if cleaned_buffer.strip().startswith('{'):
                        json_attempts += 1
                        try:
                            import json
                            parsed_json = json.loads(cleaned_buffer)
                            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–ª–Ω—ã–π JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Ü–µ–ª–∏–∫–æ–º
                            if isinstance(parsed_json, dict):
                                logger.debug(f"‚úÖ Legacy: –ü–æ–ª–Ω—ã–π JSON –Ω–∞–∫–æ–ø–ª–µ–Ω: {len(json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                                yielded_any = True
                                yield json.dumps(parsed_json, ensure_ascii=False)
                                json_buffer = ""
                                json_attempts = 0
                                continue
                        except (json.JSONDecodeError, ValueError):
                            # JSON –µ—â—ë –Ω–µ –ø–æ–ª–Ω—ã–π - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å
                            if json_attempts >= MAX_JSON_ATTEMPTS:
                                # –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
                                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON ({MAX_JSON_ATTEMPTS}), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                                yielded_any = True
                                yield json_buffer
                                json_buffer = ""
                                json_attempts = 0
                                continue
                            logger.debug(f"üì¶ Legacy: –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON: {len(json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {json_attempts}/{MAX_JSON_ATTEMPTS})")
                            continue
                    
                    # –ï—Å–ª–∏ –Ω–µ JSON –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                    if not json_buffer.strip().startswith('{'):
                        yielded_any = True
                        logger.debug(f"üì® Legacy TextProcessor sentence: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                        yield sentence
                        json_buffer = ""
                        json_attempts = 0
                
                # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞
                if json_buffer:
                    logger.debug(f"üì¶ Legacy: –û—Å—Ç–∞–ª—Å—è –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä ({len(json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                    yielded_any = True
                    yield json_buffer
            except Exception as processing_error:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ legacy TextProcessor: {processing_error}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                import traceback
                traceback.print_exc()

        if not yielded_any:
            logger.debug("‚ö†Ô∏è TextProcessor –Ω–µ –≤–µ—Ä–Ω—É–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–∞–∑–±–∏–≤–∫—É")
            for fallback_sentence in self._split_into_sentences(enriched_text):
                if fallback_sentence:
                    yield fallback_sentence

    async def _sanitize_for_tts(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return ""

        if self.text_filter_module and hasattr(self.text_filter_module, 'process'):
            try:
                result = await self.text_filter_module.process({
                    "operation": "clean_text",
                    "text": text,
                    "options": {
                        "remove_special_chars": True,
                        "remove_extra_whitespace": True,
                        "normalize_unicode": True,
                        "remove_control_chars": True
                    }
                })
                if isinstance(result, dict) and result.get("success") and result.get("cleaned_text") is not None:
                    return result.get("cleaned_text", "").strip()
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextFilterModule: %s", err)

        return text.strip()

    async def _split_complete_sentences(self, text: str) -> tuple[list[str], str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            logger.debug("‚ö†Ô∏è _split_complete_sentences: text –ø—É—Å—Ç–æ–π")
            return [], ""

        if self.text_filter_module and hasattr(self.text_filter_module, 'process'):
            try:
                result = await self.text_filter_module.process({
                    "operation": "split_sentences",
                    "text": text
                })
                if isinstance(result, dict) and result.get("success"):
                    sentences = result.get("sentences", [])
                    remainder = result.get("remainder", "")
                    logger.debug(f"‚úÖ TextFilterModule –≤–µ—Ä–Ω—É–ª: sentences={len(sentences)}, remainder_len={len(remainder)}")
                    return sentences, remainder
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextFilterModule: %s", err)

        # Fallback: –µ—Å–ª–∏ text_filter_module –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        stripped = text.strip()
        result = ([stripped] if stripped else [], "")
        logger.debug(f"üìù Fallback _split_complete_sentences: text_len={len(text)}, stripped_len={len(stripped)}, sentences={len(result[0])}")
        return result

    async def _count_meaningful_words(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á—ë—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return 0

        if self.text_filter_module and hasattr(self.text_filter_module, 'process'):
            try:
                result = await self.text_filter_module.process({
                    "operation": "count_meaningful_words",
                    "text": text
                })
                if isinstance(result, dict) and result.get("success"):
                    return int(result.get("count", 0))
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ TextFilterModule: %s", err)

        return len([w for w in text.split() if w.strip()])

    async def _stream_text_module(self, text: str, screenshot_data: Optional[str]):
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è."""
        payload: Dict[str, Any] = {"text": text}
        if screenshot_data:
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64 (WebP)
            payload["image_data"] = screenshot_data

        async for chunk in self._stream_module_results(self.text_module, payload):
            yield chunk

    async def _stream_audio_module(self, text: str):
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ –º–æ–¥—É–ª—è."""
        async for chunk in self._stream_module_results(self.audio_module, {"text": text}):
            yield chunk

    async def _stream_module_results(self, module, payload: Dict[str, Any]):
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–∑–æ–≤ module.process —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π async generator."""
        if not module or not hasattr(module, 'process'):
            return
        try:
            result = await module.process(payload)
            if result is None:
                return
            if hasattr(result, "__aiter__"):
                async for item in result:
                    yield item
            else:
                yield result
        except Exception as err:
            logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è %s: %s", getattr(module, 'name', 'unknown'), err)

    def _extract_text_chunk(self, chunk: Any) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥—É–ª—è.
        
        –í–ê–ñ–ù–û: –î–ª—è action-–æ—Ç–≤–µ—Ç–æ–≤ (—Å command) –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ü–û–õ–ù–´–ô JSON,
        –∞ –Ω–µ —Ç–æ–ª—å–∫–æ text, —á—Ç–æ–±—ã –ø–∞—Ä—Å–µ—Ä –º–æ–≥ –∏–∑–≤–ª–µ—á—å command_payload.
        
        TextProcessingModule –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'text': chunk, 'type': 'text_chunk'},
        –≥–¥–µ chunk - —ç—Ç–æ —Ç–µ–∫—Å—Ç –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ JSON).
        """
        if chunk is None:
            return ""
        if isinstance(chunk, str):
            chunk_stripped = chunk.strip()
            # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω–∞ –∫–æ–º–∞–Ω–¥—É
            if chunk_stripped.startswith('{'):
                try:
                    import json
                    parsed = json.loads(chunk_stripped)
                    if isinstance(parsed, dict):
                        # –ï—Å–ª–∏ —ç—Ç–æ action-–æ—Ç–≤–µ—Ç —Å –∫–æ–º–∞–Ω–¥–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ü–û–õ–ù–´–ô JSON
                        if 'command' in parsed:
                            logger.debug(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω action-–æ—Ç–≤–µ—Ç –≤ chunk: command={parsed.get('command')}")
                            return chunk_stripped  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–π JSON
                        # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ text
                        elif 'text' in parsed:
                            return str(parsed['text'])
                except (json.JSONDecodeError, ValueError):
                    # –ù–µ –ø–æ–ª–Ω—ã–π JSON –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
                    pass
            return chunk
        if isinstance(chunk, dict):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: text -> text_response -> value -> chunk
            for key in ("text", "text_response", "value", "chunk"):
                value = chunk.get(key)
                if value is not None:
                    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ—ë –Ω–∞–ø—Ä—è–º—É—é
                    # –ù–ï –ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å JSON, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç
                    if isinstance(value, str):
                        # –ï—Å–ª–∏ —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω–∞ –∫–æ–º–∞–Ω–¥—É
                        value_stripped = value.strip()
                        if value_stripped.startswith('{'):
                            try:
                                import json
                                parsed = json.loads(value_stripped)
                                if isinstance(parsed, dict):
                                    # –ï—Å–ª–∏ —ç—Ç–æ action-–æ—Ç–≤–µ—Ç —Å –∫–æ–º–∞–Ω–¥–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ü–û–õ–ù–´–ô JSON
                                    if 'command' in parsed:
                                        logger.debug(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω action-–æ—Ç–≤–µ—Ç –≤ dict value: command={parsed.get('command')}")
                                        return value_stripped  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–π JSON
                                    # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ text
                                    elif 'text' in parsed:
                                        return str(parsed['text'])
                            except (json.JSONDecodeError, ValueError):
                                # –ù–µ –ø–æ–ª–Ω—ã–π JSON –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è
                                pass
                        return value
                    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ - dict/list, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ JSON —Å—Ç—Ä–æ–∫—É
                    if isinstance(value, (dict, list)):
                        import json
                        return json.dumps(value, ensure_ascii=False)
                    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ - –Ω–µ —Å—Ç—Ä–æ–∫–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    return str(value)
            # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π, –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ JSON —Å—Ç—Ä–æ–∫—É
            try:
                import json
                return json.dumps(chunk, ensure_ascii=False)
            except:
                return str(chunk)
        return str(chunk) if chunk else ""

    def _extract_audio_chunk(self, chunk: Any) -> bytes:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ –±–∞–π—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥—É–ª—è."""
        if chunk is None:
            return b""
        if isinstance(chunk, (bytes, bytearray)):
            return bytes(chunk)
        if isinstance(chunk, dict):
            for key in ("audio", "audio_chunk", "data", "value"):
                value = chunk.get(key)
                if isinstance(value, (bytes, bytearray)):
                    return bytes(value)
        return b""

    def _enrich_with_memory(self, text: str, memory_context: Optional[Dict[str, Any]]) -> str:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            memory_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏
        """
        if not memory_context:
            return text
        
        try:
            memory_info = memory_context.get('recent_context', '') if memory_context else ''
            if memory_info:
                enriched_text = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {memory_info}\n\n{text}"
                logger.debug("–¢–µ–∫—Å—Ç –æ–±–æ–≥–∞—â–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏")
                return enriched_text
            return text
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç—å—é: {e}")
            return text

    async def _stream_audio_for_sentence(self, sentence: str, sentence_index: int) -> AsyncGenerator[bytes, None]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å—Ç—Ä–∏–º–∏—Ç —á–∞–Ω–∫–∏ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –∞—É–¥–∏–æ –ø–æ –º–µ—Ä–µ –∏—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency.
        
        Args:
            sentence: –¢–µ–∫—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ
            sentence_index: –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            
        Yields:
            –ß–∞–Ω–∫–∏ –∞—É–¥–∏–æ (–ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
        """
        if not sentence.strip():
            return
        if not self.audio_module:
            logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ")
            return
        
        # –°—Ç—Ä–∏–º–∏–º —á–∞–Ω–∫–∏ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency
        try:
            if hasattr(self.audio_module, 'process'):
                logger.debug(f"üîä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                chunk_count = 0
                async for chunk in self._stream_audio_module(sentence):
                    audio_chunk = self._extract_audio_chunk(chunk)
                    if audio_chunk:
                        chunk_count += 1
                        logger.debug(f"üîä Audio chunk #{chunk_count} –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(audio_chunk)} bytes")
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É, –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è
                        yield audio_chunk
                logger.debug(f"‚úÖ –ê—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {chunk_count} —á–∞–Ω–∫–æ–≤")
            elif hasattr(self.audio_module, 'generate_speech_streaming'):
                # Legacy fallback
                logger.debug(f"üîä Legacy –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                chunk_count = 0
                async for audio_chunk in self.audio_module.generate_speech_streaming(sentence):
                    if audio_chunk:
                        chunk_count += 1
                        logger.debug(f"üîä Legacy audio chunk #{chunk_count} –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(audio_chunk)} bytes")
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É, –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è
                        yield audio_chunk
                logger.debug(f"‚úÖ Legacy –∞—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {chunk_count} —á–∞–Ω–∫–æ–≤")
                
        except Exception as audio_error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {audio_error}")
            raise
    
    async def _parse_assistant_response(self, response: Union[str, Dict[str, Any]], session_id: str):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è text –∏ command_payload (–§–∞–∑–∞ 2)
        
        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å)
            session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            ParsedResponse —Å text_response –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º command_payload
        """
        try:
            config = get_config()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏—á–∞-—Ñ–ª–∞–≥ –∏ kill-switch
            if (not config.features.forward_assistant_actions or 
                config.kill_switches.disable_forward_assistant_actions):
                # –§–∏—á–∞ –≤—ã–∫–ª—é—á–µ–Ω–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                if isinstance(response, dict):
                    return self._assistant_parser.parse(response.get('text', str(response)))
                return self._assistant_parser.parse(response)
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç, –ø–µ—Ä–µ–¥–∞–≤–∞—è session_id –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ action-–æ—Ç–≤–µ—Ç—ã
            return self._assistant_parser.parse(response, session_id=session_id)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {e}, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
            if isinstance(response, dict):
                text = response.get('text', str(response))
            else:
                text = str(response)
            return self._assistant_parser.parse(text)
    
    def _log_command_detected(self, parsed, session_id: str):
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã (–§–∞–∑–∞ 2)
        
        Args:
            parsed: ParsedResponse —Å command_payload
            session_id: ID —Å–µ—Å—Å–∏–∏
        """
        if not parsed.command_payload:
            return
        
        payload = parsed.command_payload.get('payload', {})
        command = payload.get('command', 'unknown')
        args = payload.get('args', {})
        
        log_structured(
            logger,
            logging.INFO,
            f"Command detected: {command}",
            scope="command",
            method="parse_assistant_response",
            decision="start",
            ctx={
                "session_id": session_id,
                "command": command,
                "args": args
            }
        )
    
    def _log_command_complete(self, command_payload: Optional[Dict[str, Any]], session_id: str):
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã (–§–∞–∑–∞ 2)
        
        Args:
            command_payload: Command payload –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∏–∑ ctx.pending_command_payload)
            session_id: ID —Å–µ—Å—Å–∏–∏
        """
        if not command_payload:
            return
        
        payload = command_payload.get('payload', {})
        command = payload.get('command', 'unknown')
        
        log_structured(
            logger,
            logging.INFO,
            f"Command forwarded: {command}",
            scope="command",
            method="process_request_streaming",
            decision="complete",
            ctx={
                "session_id": session_id,
                "command": command
            }
        )
    
    def _extract_json_from_markdown(self, text: str) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç Markdown-–æ–±—ë—Ä—Ç–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π JSON —Ç–µ–∫—Å—Ç.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ LLM:
        - ```json {...}```
        - ``` {...}```
        - json {...}
        - –¢–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON
        - –ß–∞—Å—Ç–∏—á–Ω—ã–π JSON (–¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è)
        - JSON —Å –ª–∏—à–Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏/–ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏
        - JSON —Å trailing commas (—É–¥–∞–ª—è—é—Ç—Å—è)
        - JSON —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ (—É–¥–∞–ª—è—é—Ç—Å—è)
        
        Args:
            text: –¢–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å JSON –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
            
        Returns:
            –ß–∏—Å—Ç—ã–π JSON —Ç–µ–∫—Å—Ç –±–µ–∑ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        """
        if not text:
            return ""

        import re
        
        text = str(text).strip()

        # –í–∞—Ä–∏–∞–Ω—Ç 1: Markdown code fence ```json ... ``` –∏–ª–∏ ``` ... ```
        if text.startswith("```"):
            # –£–¥–∞–ª—è–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–π fence
            text = text[3:]
            text = text.lstrip()
            
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫ (json/JSON/JSONC –∏ —Ç.–¥.)
            lowered = text.lower()
            if lowered.startswith("json"):
                text = text[4:]
            text = text.lstrip()
            
            # –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫–∏
            while text.startswith(("\n", "\r")):
                text = text[1:]
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π fence (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –∫–æ–Ω—Ü–µ –∏–ª–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ JSON)
            text = text.rstrip()
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()

        # –í–∞—Ä–∏–∞–Ω—Ç 2: –¢–µ–∫—Å—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "json" (–±–µ–∑ markdown)
        # –£–¥–∞–ª—è–µ–º "json" –µ—Å–ª–∏ –æ–Ω —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ–¥ JSON –æ–±—ä–µ–∫—Ç–æ–º
        text_lower = text.lower()
        if text_lower.startswith("json") and len(text) > 4:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ "json" –∏–¥—ë—Ç –ø—Ä–æ–±–µ–ª/–ø–µ—Ä–µ–Ω–æ—Å –∏ –∑–∞—Ç–µ–º {
            after_json = text[4:].lstrip()
            if after_json.startswith("{") or after_json.startswith("\n{") or after_json.startswith("\r{"):
                text = after_json

        # –í–∞—Ä–∏–∞–Ω—Ç 3: –¢–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON - –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ JSON –æ–±—ä–µ–∫—Ç
        # –ò—â–µ–º –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é
        first_brace = text.find("{")
        last_brace = text.rfind("}")
        
        if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –æ–±—ä–µ–∫—Ç
            json_candidate = text[first_brace:last_brace + 1]
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤–æ–∫—Ä—É–≥
            json_candidate = json_candidate.strip()
            
            # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã:
            # 1. –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (// –∏ /* */) - —Ö–æ—Ç—è JSON –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç, LLM –º–æ–∂–µ—Ç –∏—Ö –¥–æ–±–∞–≤–∏—Ç—å
            json_candidate = re.sub(r'//.*?$', '', json_candidate, flags=re.MULTILINE)  # –û–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            json_candidate = re.sub(r'/\*.*?\*/', '', json_candidate, flags=re.DOTALL)  # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            
            # 2. –£–¥–∞–ª—è–µ–º trailing commas –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–º–∏ —Å–∫–æ–±–∫–∞–º–∏/—Ñ–∏–≥—É—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏
            json_candidate = re.sub(r',\s*}', '}', json_candidate)  # Trailing comma –ø–µ—Ä–µ–¥ }
            json_candidate = re.sub(r',\s*]', ']', json_candidate)  # Trailing comma –ø–µ—Ä–µ–¥ ]
            
            # 3. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            json_candidate = re.sub(r'\n\s*\n', '\n', json_candidate)  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            json_candidate = re.sub(r'[ \t]+', ' ', json_candidate)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            
            # 4. –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –¥–≤–æ–µ—Ç–æ—á–∏–π –∏ –∑–∞–ø—è—Ç—ã—Ö
            json_candidate = re.sub(r'\s*:\s*', ': ', json_candidate)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ :
            json_candidate = re.sub(r'\s*,\s*', ', ', json_candidate)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ ,
            
            return json_candidate

        # –ï—Å–ª–∏ JSON –æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        # (–º–æ–∂–µ—Ç –±—ã—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π JSON –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è)
        return text.strip()

    def _split_into_sentences(self, text: str) -> list[str]:
        """
        –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
            import re
            sentences = re.split(r'[.!?]+', text)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            clean_sentences = [s.strip() for s in sentences if s.strip()]
            
            logger.debug(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(clean_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            return clean_sentences
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return [text]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            logger.info("–û—á–∏—Å—Ç–∫–∞ StreamingWorkflowIntegration...")
            self.is_initialized = False
            logger.info("‚úÖ StreamingWorkflowIntegration –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ StreamingWorkflowIntegration: {e}")
