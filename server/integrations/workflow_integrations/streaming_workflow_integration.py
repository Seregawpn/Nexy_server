#!/usr/bin/env python3
"""
StreamingWorkflowIntegration - —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º: —Ç–µ–∫—Å—Ç ‚Üí –∞—É–¥–∏–æ ‚Üí –∫–ª–∏–µ–Ω—Ç
"""

import logging
import asyncio
from typing import Dict, Any, AsyncGenerator, Optional, Union, Set
from datetime import datetime
from dataclasses import dataclass, field

from config.unified_config import WorkflowConfig, get_config
from integrations.core.assistant_response_parser import AssistantResponseParser
from integrations.core.json_stream_extractor import JsonStreamExtractor
from modules.session_management.core.session_registry import SessionRegistry
from utils.logging_formatter import log_structured

logger = logging.getLogger(__name__)


@dataclass
class RequestContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    session_id: str
    stream_buffer: str = ""
    pending_segment: str = ""
    processed_sentences: Set[int] = field(default_factory=set)
    json_buffer: str = ""
    pending_command_payload: Optional[Dict[str, Any]] = None
    command_payload_sent: bool = False
    json_parsed: bool = False
    has_emitted: bool = False
    emitted_segment_counter: int = 0
    captured_segments: list[str] = field(default_factory=list)
    sentence_audio_map: Dict[int, int] = field(default_factory=dict)
    total_audio_chunks: int = 0
    total_audio_bytes: int = 0
    # [STREAMING] –ü–æ—Ç–æ–∫–æ–≤—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ –∏–∑ JSON
    json_extractor: Optional["JsonStreamExtractor"] = None


class StreamingWorkflowIntegration:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ‚Üí –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ ‚Üí —Å—Ç—Ä–∏–º–∏–Ω–≥ –∫–ª–∏–µ–Ω—Ç—É
    """
    
    def __init__(
        self,
        text_processor=None,
        audio_processor=None,
        memory_workflow=None,
        text_filter_manager=None,
        workflow_config: Optional[Union[WorkflowConfig, Dict[str, Any]]] = None,
        coordinator=None,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StreamingWorkflowIntegration
        
        Args:
            text_processor: –ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (UniversalModuleInterface)
            audio_processor: –ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ (UniversalModuleInterface)
            memory_workflow: Workflow –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é
            text_filter_manager: –ú–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (UniversalModuleInterface)
        """
        # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ (–Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.text_module = text_processor
        self.audio_module = audio_processor
        self.memory_workflow = memory_workflow
        self.text_filter_module = text_filter_manager
        self.coordinator = coordinator  # –î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ browser_use –º–æ–¥—É–ª—é
        self._database_manager = None
        self.is_initialized = False
        
        # –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Ç–µ–ø–µ—Ä—å –≤ RequestContext (request-scoped), –Ω–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        # –£–¥–∞–ª–µ–Ω—ã: self._stream_buffer, self._has_emitted, self._pending_segment, 
        #          self._processed_sentences, self._pending_command_payload, 
        #          self._command_payload_sent, self._json_buffer, self._json_parsed
        
        self._assistant_parser = AssistantResponseParser()
        
        # –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        if workflow_config is None:
            workflow_config = get_config().get_workflow_thresholds()

        if isinstance(workflow_config, WorkflowConfig):
            cfg = workflow_config
        else:
            cfg = WorkflowConfig(**workflow_config)

        self.stream_min_chars: int = cfg.stream_min_chars
        self.stream_min_words: int = cfg.stream_min_words
        self.stream_first_sentence_min_words: int = cfg.stream_first_sentence_min_words
        self.stream_punct_flush_strict: bool = bool(cfg.stream_punct_flush_strict)
        self.force_flush_max_chars: int = cfg.force_flush_max_chars
        self.sentence_joiner: str = " "
        self.end_punctuations = ('.', '!', '?')
        
        # Single-flight –∑–∞—â–∏—Ç–∞: lock + —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π SessionRegistry.
        self._inflight_lock = asyncio.Lock()
        self._session_registry = SessionRegistry()
        
        # Guard –ø–æ hardware_id (—É—Å–ª–æ–≤–Ω—ã–π, —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥)
        # –ï—Å–ª–∏ prevent_concurrent_hardware_id_sessions=True, –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–µ—Å—Å–∏–∏ –æ–¥–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        # –ï—Å–ª–∏ False (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é), –¥–æ–ø—É—Å–∫–∞—é—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Å–µ—Å—Å–∏–∏ –æ–¥–Ω–æ–≥–æ hardware_id
        self._prevent_concurrent_hardware_id_sessions: bool = bool(cfg.prevent_concurrent_hardware_id_sessions)
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞
        logger.info(
            f"üîß StreamingWorkflowIntegration —Å–æ–∑–¥–∞–Ω: instance_id={id(self)}",
            extra={
                'scope': 'workflow',
                'method': '__init__',
                'instance_id': id(self),
                'registry_id': id(self._session_registry)
            }
        )
    
    async def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StreamingWorkflowIntegration...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥—É–ª–µ–π
            if not self.text_module:
                logger.warning("‚ö†Ô∏è TextProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.audio_module:
                logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.memory_workflow:
                logger.warning("‚ö†Ô∏è MemoryWorkflow –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.text_filter_module:
                logger.warning("‚ö†Ô∏è TextFilterManager –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            self.is_initialized = True
            logger.info("‚úÖ StreamingWorkflowIntegration –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ StreamingWorkflowIntegration: {e}")
            return False

    def set_database_manager(self, database_manager) -> None:
        """–í–Ω–µ–¥—Ä–µ–Ω–∏–µ DatabaseManager (single owner-path —á–µ—Ä–µ–∑ GrpcServiceManager)."""
        self._database_manager = database_manager
        logger.info("‚úÖ DatabaseManager wired to StreamingWorkflowIntegration")
    
    async def _process_text_for_tts(self, text_chunk: str, ctx: RequestContext) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Processes a text chunk for TTS emission: buffers, splits into sentences, checks thresholds, and yields events.
        Mutates ctx state.
        """
        # [FIX] Defensive check: Detect if text_chunk looks like JSON/Metadata leakage
        if text_chunk and (text_chunk.strip().startswith('{') or text_chunk.strip().startswith('[')):
            chunk_stripped = text_chunk.strip()
            # Only try to parse if it looks like a complex structure, not just a quote starting with {
            if len(chunk_stripped) > 2:
                import json
                try:
                    logger.debug(f"üîç Checking potential JSON leakage in TTS input: '{chunk_stripped[:50]}...'")
                    parsed = json.loads(chunk_stripped)
                    
                    # If it's a dict, try to extract 'text'
                    if isinstance(parsed, dict):
                        if 'text' in parsed and isinstance(parsed['text'], str):
                            logger.info(f"‚úÖ Extracted text from leaked JSON: '{parsed['text'][:50]}...'")
                            text_chunk = parsed['text']
                        elif 'text_response' in parsed and isinstance(parsed['text_response'], str):
                             logger.info(f"‚úÖ Extracted text_response from leaked JSON: '{parsed['text_response'][:50]}...'")
                             text_chunk = parsed['text_response']
                        else:
                            # If no text field found, it might be metadata/command only -> suppress
                            logger.warning(f"‚ö†Ô∏è Suppressing JSON metadata in TTS (no text field): '{chunk_stripped[:50]}...'")
                            text_chunk = ""
                            
                    # If it's a list, it might be a history dump -> suppress
                    elif isinstance(parsed, list):
                        logger.warning(f"‚ö†Ô∏è Suppressing JSON list in TTS: '{chunk_stripped[:50]}...'")
                        text_chunk = ""
                        
                except (json.JSONDecodeError, ValueError):
                    # Not valid JSON, proceed as text
                    pass

        # Sanitize and buffer
        sanitized = await self._sanitize_for_tts(text_chunk)
        if sanitized:
            ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{sanitized}" if ctx.stream_buffer else sanitized)
            logger.debug(f"üì¶ stream_buffer –æ–±–Ω–æ–≤–ª–µ–Ω: {len(ctx.stream_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            logger.warning(f"‚ö†Ô∏è sanitized –ø—É—Å—Ç–æ–π –¥–ª—è chunk: '{text_chunk[:50]}...'")

        # Split into complete sentences
        logger.debug(f"üîç –í—ã–∑–æ–≤ _split_complete_sentences —Å stream_buffer: {len(ctx.stream_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
        complete_sentences, remainder = await self._split_complete_sentences(ctx.stream_buffer)
        logger.debug(f"‚úÖ _split_complete_sentences –≤–µ—Ä–Ω—É–ª: {len(complete_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, remainder={len(remainder) if remainder else 0} —Å–∏–º–≤–æ–ª–æ–≤")
        ctx.stream_buffer = remainder

        for complete in complete_sentences:
            # Aggregate short sentences
            candidate = complete if not ctx.pending_segment else f"{ctx.pending_segment}{self.sentence_joiner}{complete}"
            words_count = await self._count_meaningful_words(candidate)
            
            should_emit = (not ctx.has_emitted and (words_count >= self.stream_first_sentence_min_words or len(candidate) >= self.stream_min_chars)) or \
               (ctx.has_emitted and (words_count >= self.stream_min_words or len(candidate) >= self.stream_min_chars))
            
            if should_emit:
                # [FIX] Deduplication logic - now checks ALL non-empty texts (removed len > 10 limit)
                to_emit = candidate.strip()
                if to_emit:
                    complete_hash = hash(to_emit)
                    if complete_hash in ctx.processed_sentences:
                        logger.warning(f"üîÑ –ü–†–û–ü–£–°–ö–ê–ï–ú –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: '{to_emit[:50]}...' (hash={complete_hash})")
                        continue
                    ctx.processed_sentences.add(complete_hash)
                
                # Emit
                ctx.emitted_segment_counter += 1
                ctx.pending_segment = ""
                ctx.has_emitted = True
                ctx.captured_segments.append(to_emit)
                
                yield {
                    'success': True,
                    'text_response': to_emit,
                    'sentence_index': ctx.emitted_segment_counter
                }

                # Generate Audio
                if to_emit.strip():
                    tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                    segment_audio_chunks = 0
                    async for audio_chunk in self._stream_audio_for_sentence(tts_text, ctx.emitted_segment_counter):
                        if not audio_chunk: continue
                        segment_audio_chunks += 1
                        ctx.total_audio_chunks += 1
                        ctx.total_audio_bytes += len(audio_chunk)
                        yield {
                            'success': True,
                            'audio_chunk': audio_chunk,
                            'sentence_index': ctx.emitted_segment_counter
                        }
                    ctx.sentence_audio_map[ctx.emitted_segment_counter] = segment_audio_chunks
                else:
                    logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞—É–¥–∏–æ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ segment #{ctx.emitted_segment_counter}")
            else:
                ctx.pending_segment = candidate

    async def process_request_streaming(self, request_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º—è—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
        if not self.is_initialized:
            logger.error("‚ùå StreamingWorkflowIntegration –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            yield {
                'success': False,
                'error': 'StreamingWorkflowIntegration not initialized',
                'text_response': '',
            }
            return

        session_id = request_data.get('session_id')
        if not session_id or session_id == 'unknown':
            # –ö–†–ò–¢–ò–ß–ù–û: session_id –¥–æ–ª–∂–µ–Ω –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –∏–∑ gRPC —Å–ª–æ—è (–∫–ª–∏–µ–Ω—Ç—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç)
            logger.error(
                f"‚ùå session_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ä–∞–≤–µ–Ω 'unknown' - –Ω–∞—Ä—É—à–µ–Ω–∏–µ Source of Truth",
                extra={
                    'scope': 'workflow',
                    'method': 'process_request_streaming',
                    'decision': 'error',
                    'ctx': {'session_id': session_id, 'reason': 'missing_session_id'}
                }
            )
            yield {
                'success': False,
                'error': 'session_id must be provided by gRPC layer',
                'error_code': 'INVALID_ARGUMENT',
                'error_type': 'missing_session_id',
                'text_response': '',
            }
            return

        # –ö–†–ò–¢–ò–ß–ù–û: –ü–æ–ª—É—á–∞–µ–º hardware_id –¥–ª—è guard –ø—Ä–æ–≤–µ—Ä–∫–∏
        hardware_id = request_data.get('hardware_id')
        if not hardware_id or hardware_id.strip() == "" or hardware_id.lower() == "unknown":
            logger.error(
                f"‚ùå hardware_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ä–∞–≤–µ–Ω 'unknown' - –Ω–∞—Ä—É—à–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞",
                extra={
                    'scope': 'workflow',
                    'method': 'process_request_streaming',
                    'decision': 'error',
                    'ctx': {'hardware_id': hardware_id, 'reason': 'invalid_hardware_id'}
                }
            )
            yield {
                'success': False,
                'error': 'hardware_id must be provided and valid (not empty or "unknown")',
                'error_code': 'INVALID_ARGUMENT',
                'error_type': 'invalid_hardware_id',
                'text_response': '',
            }
            return
        
        # ‚≠ê SUBSCRIPTION GATE: –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Ç–æ—á–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–∞
        # Feature ID: F-2025-017-stripe-payment
        from modules.subscription import get_subscription_module
        
        subscription_module = get_subscription_module()
        if subscription_module:
            gate_result = await subscription_module.can_process(hardware_id)
            
            if not gate_result.allowed:
                logger.info(
                    f"[F-2025-017] subscription_gate=deny reason={gate_result.reason} "
                    f"hardware_id={hardware_id[:8]}... session_id={session_id}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'decision': 'deny',
                        'feature_id': 'F-2025-017',
                        'ctx': {
                            'hardware_id': hardware_id,
                            'session_id': session_id,
                            'reason': gate_result.reason,
                            'status': gate_result.status
                        }
                    }
                )
                yield {
                    'success': False,
                    'error': gate_result.message or 'Access denied by subscription gate',
                    'error_code': 'PERMISSION_DENIED',
                    'error_type': 'subscription_gate_denied',
                    'subscription_status': gate_result.status,
                    'subscription_reason': gate_result.reason,
                    'text_response': '',
                }
                return
            else:
                logger.info(
                    f"[F-2025-017] subscription_gate=allow reason={gate_result.reason} "
                    f"hardware_id={hardware_id[:8]}... session_id={session_id}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'decision': 'allow',
                        'feature_id': 'F-2025-017',
                        'ctx': {
                            'hardware_id': hardware_id,
                            'session_id': session_id,
                            'reason': gate_result.reason,
                            'status': gate_result.status
                        }
                    }
                )
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–¥–ø–∏—Å–∫–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            subscription_context = gate_result.subscription_context
            
        else:
            subscription_context = None
        
        # –°–û–ó–î–ê–ï–ú request-scoped –∫–æ–Ω—Ç–µ–∫—Å—Ç

        ctx = RequestContext(session_id=session_id)
        
        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ single-flight –ø—Ä–æ–≤–µ—Ä–∫–æ–π
        logger.info(
            f"üîç Single-flight check: session_id={session_id}, hardware_id={hardware_id}, instance_id={id(self)}",
            extra={
                'scope': 'workflow',
                'method': 'process_request_streaming',
                'session_id': session_id,
                'hardware_id': hardware_id,
                'instance_id': id(self),
                'current_inflight': self._session_registry.get_inflight_session_ids()
            }
        )
        
        # Atomic single-flight: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥ –æ–¥–Ω–∏–º lock
        async with self._inflight_lock:
            acquire_result = self._session_registry.try_acquire_inflight(
                session_id=session_id,
                hardware_id=hardware_id,
                prevent_concurrent_hardware=self._prevent_concurrent_hardware_id_sessions,
            )

            if not acquire_result.get('ok'):
                reason = acquire_result.get('reason', 'concurrent_request')
                active_sessions = acquire_result.get('active_sessions', [])
                if reason == 'concurrent_hardware_id':
                    logger.warning(
                        f"‚ö†Ô∏è –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å hardware_id={hardware_id} –æ—Ç–∫–ª–æ–Ω—ë–Ω (single-flight –ø–æ hardware_id) - "
                        f"–∞–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏: {active_sessions}",
                        extra={
                            'scope': 'workflow',
                            'method': 'process_request_streaming',
                            'decision': 'reject',
                            'ctx': {
                                'hardware_id': hardware_id,
                                'session_id': session_id,
                                'reason': reason,
                                'active_sessions': active_sessions
                            }
                        }
                    )
                    yield {
                        'success': False,
                        'error': f'Concurrent request for hardware_id={hardware_id} is not allowed (active sessions: {active_sessions})',
                        'error_code': 'RESOURCE_EXHAUSTED',
                        'error_type': reason,
                        'text_response': '',
                    }
                    return

                logger.warning(
                    f"‚ö†Ô∏è –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å session_id={session_id} –æ—Ç–∫–ª–æ–Ω—ë–Ω (single-flight)",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'decision': 'reject',
                        'ctx': {'session_id': session_id, 'reason': reason}
                    }
                )
                yield {
                    'success': False,
                    'error': f'Concurrent request for session_id={session_id} is not allowed',
                    'error_code': 'RESOURCE_EXHAUSTED',
                    'error_type': reason,
                    'text_response': '',
                }
                return

            logger.info(
                f"‚úÖ Session –¥–æ–±–∞–≤–ª–µ–Ω –≤ inflight: session_id={session_id}, hardware_id={hardware_id}, instance_id={id(self)}",
                extra={
                    'scope': 'workflow',
                    'method': 'process_request_streaming',
                    'session_id': session_id,
                    'hardware_id': hardware_id,
                    'instance_id': id(self),
                    'current_inflight': self._session_registry.get_inflight_session_ids(),
                    'action': 'added_to_inflight'
                }
            )
        
        try:
            import time
            request_start_time = time.time()
            
            logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: session_id={session_id}, hardware_id={hardware_id}")
            
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è request_data
            prompt_text = request_data.get('text', '') or ''
            prompt_text_stripped = prompt_text.strip()
            
            logger.info(f"‚Üí Input text len={len(prompt_text)}, stripped_len={len(prompt_text_stripped)}, has_screenshot={bool(request_data.get('screenshot'))}")
            logger.info(f"‚Üí Input text content: '{prompt_text[:100]}...'")
            
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –ø—É—Å—Ç–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É
            if not prompt_text_stripped:
                logger.warning(
                    f"‚ö†Ô∏è –ü–£–°–¢–û–ô –ü–†–û–ú–ü–¢ –¥–ª—è session_id={session_id}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'session_id': session_id,
                        'decision': 'error',
                        'ctx': {'reason': 'empty_prompt', 'prompt_len': len(prompt_text)}
                    }
                )
                yield {
                    'success': False,
                    'error': 'Empty prompt: text field is required and cannot be empty',
                    'error_code': 'INVALID_ARGUMENT',
                    'error_type': 'empty_prompt',
                    'text_response': '',
                }
                return

            logger.info("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–£–õ–ï–ô:")
            logger.info(f"   ‚Üí text_processor: {self.text_module is not None}")
            logger.info(f"   ‚Üí audio_processor: {self.audio_module is not None}")
            if self.text_module:
                logger.info(f"   ‚Üí text_processor.is_initialized: {getattr(self.text_module, 'is_initialized', 'NO_ATTR')}")
            if self.audio_module:
                logger.info(f"   ‚Üí audio_processor.is_initialized: {getattr(self.audio_module, 'is_initialized', 'NO_ATTR')}")

            # –ö–†–ò–¢–ò–ß–ù–û: hardware_id —É–∂–µ –ø–æ–ª—É—á–µ–Ω –∏ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω –≤—ã—à–µ (–≤ guard –ø—Ä–æ–≤–µ—Ä–∫–µ)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ hardware_id
            if hardware_id != 'unknown' and self.memory_workflow:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –≤ —Ñ–æ–Ω–µ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É)
                asyncio.create_task(
                    self.memory_workflow.prefetch_memory(hardware_id)
                )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞–º—è—Ç—å (–∏–∑ –∫—ç—à–∞ –∏–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º)
            memory_start_time = time.time()
            memory_context = await self._get_memory_context_parallel(hardware_id)
            memory_time = (time.time() - memory_start_time) * 1000
            memory_size = len(str(memory_context)) if memory_context else 0
            logger.info(f"‚è±Ô∏è  Memory context –ø–æ–ª—É—á–µ–Ω –∑–∞ {memory_time:.2f}ms (—Ä–∞–∑–º–µ—Ä: {memory_size} —Å–∏–º–≤–æ–ª–æ–≤)")
            MAX_JSON_BUFFER_SIZE = 10000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ (10KB)
            json_parse_attempts = 0  # –°—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
            MAX_JSON_PARSE_ATTEMPTS = 10  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON

            # –ú–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
            first_text_time = None
            first_audio_time = None
            llm_start_time = time.time()
            input_sentence_counter = 0
            
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            logger.info(
                f"üîÑ –ù–∞—á–∞–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –æ—Ç LLM: prompt_len={len(prompt_text_stripped)}",
                extra={
                    'scope': 'workflow',
                    'method': 'process_request_streaming',
                    'session_id': session_id,
                    'prompt_len': len(prompt_text_stripped)
                }
            )
            
            llm_iteration_started = False
            llm_chunks_received = 0
            parsed = None  # [FIX] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ —Ü–∏–∫–ª–æ–º –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è NameError



            async for processed_sentence in self._iter_processed_sentences(
                prompt_text_stripped,
                request_data.get('screenshot'),
                memory_context,
                subscription_context=subscription_context, # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–¥–ø–∏—Å–∫–∏
                session_id=session_id
            ):
                sentence = processed_sentence
                if not llm_iteration_started:
                    llm_iteration_started = True
                    logger.info("‚úÖ –ü–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞—á–∞–ª—Å—è: –ø–æ–ª—É—á–µ–Ω –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç")
                llm_chunks_received += 1
                if first_text_time is None:
                    first_text_time = (time.time() - llm_start_time) * 1000
                    logger.info(f"‚è±Ô∏è  –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {first_text_time:.2f}ms")
                input_sentence_counter += 1
                debug_snippet = sentence[:100].replace('\n', '\\n')
                logger.debug(f"üìù In sentence #{input_sentence_counter}: '{debug_snippet}' ({len(sentence)} chars)")

                # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
                if len(ctx.json_buffer) + len(sentence) > MAX_JSON_BUFFER_SIZE:
                    logger.warning(f"‚ö†Ô∏è JSON –±—É—Ñ–µ—Ä –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç ({MAX_JSON_BUFFER_SIZE} —Å–∏–º–≤–æ–ª–æ–≤), —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                    if ctx.json_buffer:
                        parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                        async for event in self._process_text_for_tts(parsed.text_response, ctx):
                             if 'audio_chunk' in event and first_audio_time is None:
                                 first_audio_time = (time.time() - request_start_time) * 1000
                             yield event
                    
                    parsed = await self._parse_assistant_response(sentence, session_id)
                    async for event in self._process_text_for_tts(parsed.text_response, ctx):
                         if 'audio_chunk' in event and first_audio_time is None:
                             first_audio_time = (time.time() - request_start_time) * 1000
                         yield event

                    ctx.json_buffer = ""
                    json_parse_attempts = 0
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º JSON –±–ª–æ–∫)
                else:
                    # [STREAMING] –ü–æ—Ç–æ–∫–æ–≤–æ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON —Å –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞
                    
                    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∏ –ø–µ—Ä–≤–æ–º JSON-–ø–æ–¥–æ–±–Ω–æ–º —á–∞–Ω–∫–µ
                    if ctx.json_extractor is None:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—ã—Ä–æ–π —á–∞–Ω–∫ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ JSON
                        if sentence.strip().startswith('{') or sentence.strip().startswith('```'):
                            ctx.json_extractor = JsonStreamExtractor()
                            logger.info(f"üîÑ [STREAMING] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω JsonStreamExtractor –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ JSON")
                    
                    # –ï—Å–ª–∏ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∞–∫—Ç–∏–≤–µ–Ω ‚Äî –ø–æ–¥–∞—ë–º –°–´–†–´–ï –¥–∞–Ω–Ω—ã–µ (–±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ markdown!)
                    if ctx.json_extractor is not None:
                        # [FIX] –ü–æ–¥–∞—ë–º –°–´–†–û–ô —á–∞–Ω–∫ –Ω–∞–ø—Ä—è–º—É—é –≤ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
                        # –ù–ï –æ—á–∏—â–∞–µ–º —á–µ—Ä–µ–∑ _extract_json_from_markdown - —ç—Ç–æ –ª–æ–º–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ!
                        raw_chunk = sentence
                        
                        # [FIX] –û–±—Ä—É–±–∞–µ–º —Å–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–µ–∫—Å—Ç + JSON) —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞
                        if not ctx.json_extractor.buffer and not raw_chunk.strip().startswith('{') and '{' in raw_chunk:
                            first_brace_idx = raw_chunk.find('{')
                            if first_brace_idx >= 0:
                                pre_text = raw_chunk[:first_brace_idx]
                                raw_chunk = raw_chunk[first_brace_idx:]
                                
                                if pre_text.strip():
                                    logger.info(f"üî™ [STREAMING] Splitting mixed content: '{pre_text[:50]}...'")
                                    parsed_pre = await self._parse_assistant_response(pre_text, session_id)
                                    async for event in self._process_text_for_tts(parsed_pre.text_response, ctx):
                                        if 'audio_chunk' in event and first_audio_time is None:
                                            first_audio_time = (time.time() - request_start_time) * 1000
                                        yield event
                        
                        # –ü–æ–¥–∞—ë–º –°–´–†–û–ô —á–∞–Ω–∫ –≤ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∏ –ø–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
                        new_text = ctx.json_extractor.feed(raw_chunk)
                        
                        if new_text:
                            logger.info(f"üì§ [STREAMING] –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(new_text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ JSON 'text' ‚Üí –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ TTS")
                            # –°—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ TTS ‚Äî —ç—Ç–æ –∏ –µ—Å—Ç—å —Å—Ç—Ä–∏–º–∏–Ω–≥!
                            async for event in self._process_text_for_tts(new_text, ctx):
                                if 'audio_chunk' in event and first_audio_time is None:
                                    first_audio_time = (time.time() - request_start_time) * 1000
                                yield event
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ø–æ–ª–Ω—ã–π JSON –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã
                        full_buffer = ctx.json_extractor.get_full_buffer()
                        # –û—á–∏—â–∞–µ–º –æ—Ç markdown —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –ø–∞—Ä—Å–∏–Ω–≥–µ (–Ω–µ —Ä–∞–Ω—å—à–µ!)
                        cleaned_buffer = self._extract_json_from_markdown(full_buffer)
                        try:
                            import json
                            parsed_json: Dict[str, Any] = json.loads(cleaned_buffer)
                            # JSON –≤–∞–ª–∏–¥–µ–Ω ‚Äî –ø–∞—Ä—Å–∏–º –¥–ª—è –∫–æ–º–∞–Ω–¥—ã
                            logger.info(f"‚úÖ [STREAMING] JSON –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞–∫–æ–ø–ª–µ–Ω: {len(full_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_buffer)})")
                            ctx.json_parsed = True
                            json_parse_attempts = 0
                            
                            parsed = await self._parse_assistant_response(parsed_json, session_id)
                            
                            # –¢–µ–∫—Å—Ç –£–ñ–ï –ë–´–õ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ TTS –ø–æ—Ç–æ–∫–æ–≤–æ ‚Äî –ù–ï –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ!
                            # –ù–æ –Ω–∞–º –Ω—É–∂–µ–Ω parsed –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã
                            
                            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
                            ctx.json_extractor = None
                            ctx.json_buffer = ""
                            ctx.json_parsed = False
                            
                        except (json.JSONDecodeError, ValueError):
                            # JSON –µ—â—ë –Ω–µ –ø–æ–ª–Ω—ã–π ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å
                            json_parse_attempts += 1
                            if json_parse_attempts >= MAX_JSON_PARSE_ATTEMPTS:
                                logger.warning(f"‚ö†Ô∏è [STREAMING] –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ ({MAX_JSON_PARSE_ATTEMPTS}), —Å–±—Ä–∞—Å—ã–≤–∞–µ–º")
                                ctx.json_extractor = None
                                ctx.json_buffer = ""
                                json_parse_attempts = 0
                            else:
                                logger.debug(f"üì¶ [STREAMING] –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON: {len(full_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {json_parse_attempts}/{MAX_JSON_PARSE_ATTEMPTS})")
                                continue
                    else:
                        # –≠—Ç–æ –Ω–µ JSON ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–µ–¥–∞—ë–º —á–∞—Å—Ç—è–º–∏)
                        logger.debug(f"üìù –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–Ω–µ JSON): {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤, –ø–µ—Ä–µ–¥–∞—ë–º —á–∞—Å—Ç—è–º–∏")
                        ctx.json_buffer = ""
                        json_parse_attempts = 0
                        parsed = await self._parse_assistant_response(sentence, session_id)
                        
                        async for event in self._process_text_for_tts(parsed.text_response, ctx):
                             if 'audio_chunk' in event and first_audio_time is None:
                                 first_audio_time = (time.time() - request_start_time) * 1000
                             yield event
                
                # [FIX] –û–±—Ä–∞–±–æ—Ç–∫–∞ parsed —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –æ–ø—Ä–µ–¥–µ–ª—ë–Ω (–Ω–µ None)
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ (–¥–ª—è –æ–±–æ–∏—Ö —Å–ª—É—á–∞–µ–≤: JSON –∏ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç)
                if parsed and parsed.command_payload and not ctx.command_payload_sent:
                    command = parsed.command_payload.get('payload', {}).get('command')
                    ctx.pending_command_payload = parsed.command_payload
                    self._log_command_detected(parsed, session_id)
                
                # [FIX] –£–î–ê–õ–Å–ù –¥—É–±–ª–∏—Ä—É—é—â–∏–π –±–ª–æ–∫ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∫–æ–º–∞–Ω–¥—ã (—Å—Ç—Ä–æ–∫–∏ 662-692)
                # –¢–µ–∫—Å—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —á–µ—Ä–µ–∑ _process_text_for_tts –≤—ã—à–µ, –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞

                # Legacy TTS logic replaced by _process_text_for_tts calls above

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–ª–∞—à: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è JSON –±—É—Ñ–µ—Ä, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if ctx.json_buffer and not ctx.json_parsed:
                import json
                # –û—á–∏—â–∞–µ–º –æ—Ç markdown –ø–µ—Ä–µ–¥ –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                cleaned_buffer = self._extract_json_from_markdown(ctx.json_buffer)
                is_potential_json = cleaned_buffer.strip().startswith('{')
                if is_potential_json:
                    try:
                        parsed_json = json.loads(cleaned_buffer)
                        logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –±—É—Ñ–µ—Ä–∞: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned_buffer)})")
                        parsed = await self._parse_assistant_response(parsed_json, session_id)
                        if parsed.command_payload and not ctx.command_payload_sent:
                            ctx.pending_command_payload = parsed.command_payload
                            self._log_command_detected(parsed, session_id)
                        # –î–æ–±–∞–≤–ª—è–µ–º text_response –≤ stream_buffer –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        if parsed.text_response:
                            ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{parsed.text_response}" if ctx.stream_buffer else parsed.text_response)
                        ctx.json_buffer = ""
                        ctx.json_parsed = False
                    except (json.JSONDecodeError, ValueError):
                        # JSON –Ω–µ –≤–∞–ª–∏–¥–µ–Ω - –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                        logger.debug(f"‚ö†Ô∏è JSON –±—É—Ñ–µ—Ä –Ω–µ –≤–∞–ª–∏–¥–µ–Ω, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                        if ctx.json_buffer.strip():
                            # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ JSON - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                            parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                            if parsed.text_response:
                                ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{parsed.text_response}" if ctx.stream_buffer else parsed.text_response)
                        ctx.json_buffer = ""
                else:
                    # –≠—Ç–æ –Ω–µ JSON - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                    logger.debug(f"üìù –§–∏–Ω–∞–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä - –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(ctx.json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                    if ctx.json_buffer.strip():
                        parsed = await self._parse_assistant_response(ctx.json_buffer, session_id)
                        if parsed.text_response:
                            ctx.stream_buffer = (f"{ctx.stream_buffer}{self.sentence_joiner}{parsed.text_response}" if ctx.stream_buffer else parsed.text_response)
                    ctx.json_buffer = ""
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–ª–∞—à: —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞
            # –í–ê–ñ–ù–û: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ stream_buffer JSON-–æ–±—ä–µ–∫—Ç–æ–º
            if ctx.stream_buffer:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ stream_buffer JSON-–æ–±—ä–µ–∫—Ç–æ–º
                stream_cleaned = self._extract_json_from_markdown(ctx.stream_buffer)
                if stream_cleaned.strip().startswith('{'):
                    try:
                        import json
                        parsed_json = json.loads(stream_cleaned)
                        logger.info(f"‚úÖ JSON –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ stream_buffer –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Ñ–ª–∞—à–µ: {len(ctx.stream_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                        parsed = await self._parse_assistant_response(parsed_json, session_id)
                        if parsed.text_response:
                            ctx.stream_buffer = parsed.text_response
                            logger.info(f"üìù –ó–∞–º–µ–Ω—ë–Ω stream_buffer –Ω–∞ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π text_response: '{ctx.stream_buffer[:100]}...' (len={len(ctx.stream_buffer)})")
                    except (json.JSONDecodeError, ValueError):
                        # –ù–µ JSON –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–π - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        pass
                
                complete_sentences, remainder = await self._split_complete_sentences(ctx.stream_buffer)
                ctx.stream_buffer = remainder
                for complete in complete_sentences:
                    candidate = complete if not ctx.pending_segment else f"{ctx.pending_segment}{self.sentence_joiner}{complete}"
                    words_count = await self._count_meaningful_words(candidate)
                    # –ï—Å–ª–∏ –µ—Å—Ç—å command_payload, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —ç–º–∏—Ç–∏—Ä—É–µ–º –¥–∞–∂–µ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                    has_command = ctx.pending_command_payload and not ctx.command_payload_sent
                    should_emit = (
                        (not ctx.has_emitted and (words_count >= self.stream_first_sentence_min_words or len(candidate) >= self.stream_min_chars)) or
                        (ctx.has_emitted and (words_count >= self.stream_min_words or len(candidate) >= self.stream_min_chars)) or
                        (has_command and candidate.strip())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —ç–º–∏—Å—Å–∏—è –¥–ª—è –∫–æ–º–∞–Ω–¥
                    )
                    
                    if should_emit:
                        ctx.emitted_segment_counter += 1
                        to_emit = candidate.strip()
                        ctx.pending_segment = ""
                        ctx.has_emitted = True
                        ctx.captured_segments.append(to_emit)
                        yield {'success': True, 'text_response': to_emit, 'sentence_index': ctx.emitted_segment_counter}
                        # –§–∞–∑–∞ 2: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ text –ø—É—Å—Ç–æ–π
                        if to_emit.strip():
                            tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å—Ç—Ä–∏–º–∏–º –∞—É–¥–∏–æ —á–∞–Ω–∫–∏
                            segment_audio_chunks = 0
                            async for audio_chunk in self._stream_audio_for_sentence(tts_text, ctx.emitted_segment_counter):
                                if not audio_chunk:
                                    continue
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency
                                ctx.total_audio_chunks += 1
                                ctx.total_audio_bytes += len(audio_chunk)
                                segment_audio_chunks += 1
                                yield {'success': True, 'audio_chunk': audio_chunk, 'sentence_index': ctx.emitted_segment_counter}
                            ctx.sentence_audio_map[ctx.emitted_segment_counter] = segment_audio_chunks
                            logger.debug(f"üéß Final segment #{ctx.emitted_segment_counter} ‚Üí {segment_audio_chunks} —á–∞–Ω–∫–æ–≤, {ctx.total_audio_bytes} –±–∞–π—Ç")
                        else:
                            logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞—É–¥–∏–æ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ final segment #{ctx.emitted_segment_counter}")
                    else:
                        ctx.pending_segment = candidate
                
                # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è remainder –≤ stream_buffer, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ pending_segment
                if remainder and remainder.strip():
                    if ctx.pending_segment:
                        ctx.pending_segment = f"{ctx.pending_segment}{self.sentence_joiner}{remainder}"
                    else:
                        ctx.pending_segment = remainder

            # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç, –º–æ–∂–Ω–æ —Ñ–æ—Ä—Å-—Ñ–ª–∞—à, –µ—Å–ª–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π
            # –ò–õ–ò –µ—Å–ª–∏ –µ—Å—Ç—å command_payload (–Ω—É–∂–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ–π—Å—Ç–≤–∏—è)
            force_max = self.force_flush_max_chars
            has_command = ctx.pending_command_payload and not ctx.command_payload_sent
            should_force_flush = (
                (force_max > 0 and len(ctx.pending_segment) >= force_max) or
                (has_command and ctx.pending_segment and ctx.pending_segment.strip())
            )
            
            if should_force_flush:
                ctx.emitted_segment_counter += 1
                to_emit = ctx.pending_segment
                ctx.pending_segment = ""
                ctx.has_emitted = True
                ctx.captured_segments.append(to_emit)
                yield {'success': True, 'text_response': to_emit, 'sentence_index': ctx.emitted_segment_counter}
                # –§–∞–∑–∞ 2: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞—É–¥–∏–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ text –ø—É—Å—Ç–æ–π
                if to_emit.strip():
                    tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                    sentence_audio_chunks = 0
                    async for audio_chunk in self._stream_audio_for_sentence(tts_text, ctx.emitted_segment_counter):
                        if not audio_chunk:
                            continue
                        sentence_audio_chunks += 1
                        ctx.total_audio_chunks += 1
                        ctx.total_audio_bytes += len(audio_chunk)
                        yield {'success': True, 'audio_chunk': audio_chunk, 'sentence_index': ctx.emitted_segment_counter, 'audio_chunk_index': sentence_audio_chunks}
                    ctx.sentence_audio_map[ctx.emitted_segment_counter] = sentence_audio_chunks
                    logger.info(f"üéß Forced final segment #{ctx.emitted_segment_counter} ‚Üí audio_chunks={sentence_audio_chunks}, total_audio_chunks={ctx.total_audio_chunks}, total_bytes={ctx.total_audio_bytes}")
                else:
                    logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∞—É–¥–∏–æ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ forced segment #{ctx.emitted_segment_counter}")

            full_text = " ".join(ctx.captured_segments).strip()

            # –§–∞–∑–∞ 2: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º command_payload –æ–¥–∏–Ω —Ä–∞–∑ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –æ—Ç–≤–µ—Ç–µ
            final_result = {
                'success': True,
                'text_full_response': full_text,
                'sentences_processed': ctx.emitted_segment_counter,
                'audio_chunks_processed': ctx.total_audio_chunks,
                'audio_bytes_processed': ctx.total_audio_bytes,
                'sentence_audio_map': ctx.sentence_audio_map,
                'is_final': True
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º command_payload, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –∏ —Ñ–∏—á–∞-—Ñ–ª–∞–≥ –≤–∫–ª—é—á–µ–Ω
            if ctx.pending_command_payload and not ctx.command_payload_sent:
                config = get_config()
                if (config.features.forward_assistant_actions and 
                    not config.kill_switches.disable_forward_assistant_actions):
                    final_result['command_payload'] = ctx.pending_command_payload
                    ctx.command_payload_sent = True
                    self._log_command_complete(ctx.pending_command_payload, session_id)
                else:
                    logger.debug("–§–∏—á–∞-—Ñ–ª–∞–≥ forward_assistant_actions –≤—ã–∫–ª—é—á–µ–Ω –∏–ª–∏ kill-switch –∞–∫—Ç–∏–≤–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º command_payload")

            # –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞/–æ—Ç–≤–µ—Ç–∞ –≤ –ë–î.
            await self._persist_request_trace(
                session_id=session_id,
                hardware_id=hardware_id,
                prompt_text=prompt_text_stripped,
                full_text=full_text,
                screenshot_b64=request_data.get('screenshot'),
                emitted_segments=ctx.emitted_segment_counter,
                total_audio_chunks=ctx.total_audio_chunks,
                total_audio_bytes=ctx.total_audio_bytes,
            )

            total_time = (time.time() - request_start_time) * 1000
            
            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –ø—Ä–∏—á–∏–Ω–æ–π, –µ—Å–ª–∏ sent_any=false
            sent_any = ctx.emitted_segment_counter > 0 or ctx.total_audio_chunks > 0
            if not sent_any:
                reason = 'unknown'
                if not llm_iteration_started:
                    reason = 'llm_iteration_not_started'
                elif llm_chunks_received == 0:
                    reason = 'llm_no_chunks'
                elif ctx.emitted_segment_counter == 0:
                    reason = 'no_segments_emitted'
                elif ctx.total_audio_chunks == 0:
                    reason = 'no_audio_chunks'
                
                logger.warning(
                    f"‚ö†Ô∏è sent_any=false –¥–ª—è session_id={session_id}: reason={reason}, "
                    f"llm_iteration_started={llm_iteration_started}, llm_chunks_received={llm_chunks_received}, "
                    f"emitted_segments={ctx.emitted_segment_counter}, audio_chunks={ctx.total_audio_chunks}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'session_id': session_id,
                        'decision': 'warning',
                        'ctx': {
                            'reason': reason,
                            'llm_iteration_started': llm_iteration_started,
                            'llm_chunks_received': llm_chunks_received,
                            'emitted_segments': ctx.emitted_segment_counter,
                            'audio_chunks': ctx.total_audio_chunks,
                            'sent_any': False
                        }
                    }
                )
            else:
                logger.info(
                    f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ: segments={ctx.emitted_segment_counter}, audio_chunks={ctx.total_audio_chunks}, total_bytes={ctx.total_audio_bytes}"
                )
                
                # ‚≠ê SUBSCRIPTION USAGE: –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ü–û–°–õ–ï —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                # Feature ID: F-2025-017-stripe-payment
                if subscription_module:
                    try:
                        await subscription_module.increment_usage(hardware_id)
                        logger.debug(f"[F-2025-017] Usage incremented for {hardware_id[:8]}...")
                    except Exception as usage_error:
                        logger.warning(f"[F-2025-017] Failed to increment usage: {usage_error}")
            

            logger.info(f"‚è±Ô∏è  –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –í–†–ï–ú–ï–ù–ò:")
            logger.info(f"   ‚Ä¢ Memory context: {memory_time:.2f}ms")
            if first_text_time:
                logger.info(f"   ‚Ä¢ –î–æ –ø–µ—Ä–≤–æ–≥–æ text (LLM): {first_text_time:.2f}ms")
            if first_audio_time:
                logger.info(f"   ‚Ä¢ –î–æ –ø–µ—Ä–≤–æ–≥–æ audio (TTS): {first_audio_time:.2f}ms")
            logger.info(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}ms ({total_time/1000:.2f} —Å–µ–∫)")
            yield final_result

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ {session_id}: {e}")
            yield {
                'success': False,
                'error': str(e),
                'error_code': 'INTERNAL',
                'error_type': 'processing_error',
                'text_response': '',
            }
        finally:
            # –£–¥–∞–ª—è–µ–º session_id –∏–∑ in-flight set (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)
            async with self._inflight_lock:
                hardware_id = request_data.get('hardware_id')
                was_present = self._session_registry.release_inflight(session_id, hardware_id)
                
                logger.info(
                    f"üßπ Session —É–¥–∞–ª—ë–Ω –∏–∑ inflight: session_id={session_id}, hardware_id={hardware_id}, instance_id={id(self)}, "
                    f"was_present={was_present}",
                    extra={
                        'scope': 'workflow',
                        'method': 'process_request_streaming',
                        'session_id': session_id,
                        'hardware_id': hardware_id,
                        'instance_id': id(self),
                        'remaining_inflight': self._session_registry.get_inflight_session_ids(),
                        'action': 'removed_from_inflight',
                        'was_present': was_present
                    }
                )

    async def _persist_request_trace(
        self,
        session_id: str,
        hardware_id: str,
        prompt_text: str,
        full_text: str,
        screenshot_b64: Optional[str],
        emitted_segments: int,
        total_audio_chunks: int,
        total_audio_bytes: int,
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç request trace (session/command/answer/screenshot) –≤ –µ–¥–∏–Ω–æ–º owner-path."""
        if not self._database_manager or not getattr(self._database_manager, "is_initialized", False):
            logger.debug("DatabaseManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º request trace persistence")
            return

        try:
            user = await self._database_manager.get_user_by_hardware_id(hardware_id)
            if not user or not user.get("id"):
                logger.warning(
                    f"‚ö†Ô∏è –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è hardware_id={hardware_id}, request trace –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω"
                )
                return

            db_session_id = await self._database_manager.ensure_session(
                user_id=user["id"],
                session_id=session_id,
                metadata={
                    "hardware_id": hardware_id,
                    "source": "streaming_workflow",
                    "last_request_at": datetime.utcnow().isoformat(),
                },
            )
            if not db_session_id:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–µ—Å—Å–∏—é –≤ –ë–î –¥–ª—è session_id={session_id}")
                return

            command_id = await self._database_manager.ensure_command(
                session_id=db_session_id,
                prompt=prompt_text,
                metadata={
                    "source": "streaming_workflow",
                    "has_screenshot": bool(screenshot_b64),
                    "request_key": session_id,
                },
                language="en",
            )
            if not command_id:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É –¥–ª—è session_id={session_id}")
                return

            await self._database_manager.ensure_llm_answer(
                command_id=command_id,
                prompt=prompt_text,
                response=full_text or "",
                model_info={"provider": "langchain", "module": "text_processing"},
                performance_metrics={
                    "sentences_processed": emitted_segments,
                    "audio_chunks_processed": total_audio_chunks,
                    "audio_bytes_processed": total_audio_bytes,
                },
            )

            if screenshot_b64:
                await self._database_manager.create_screenshot(
                    session_id=db_session_id,
                    metadata={
                        "encoding": "base64",
                        "size_b64": len(screenshot_b64),
                    },
                )
        except Exception as persist_error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ persistence request trace (session_id={session_id}): {persist_error}")

    async def _get_memory_context_parallel(self, hardware_id: str) -> Optional[Dict[str, Any]]:
        """
        –ù–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏
        
        Args:
            hardware_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        """
        try:
            if not self.memory_workflow:
                logger.debug("MemoryWorkflow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
                return None
            
            import time
            start_time = time.time()
            logger.info(f"‚è±Ô∏è  –ù–∞—á–∞–ª–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è {hardware_id}")
            memory_context = await self.memory_workflow.get_memory_context_parallel(hardware_id)
            elapsed = (time.time() - start_time) * 1000
            
            if memory_context:
                context_size = len(str(memory_context))
                logger.info(f"‚è±Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.2f}ms: {len(memory_context)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, {context_size} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                logger.info(f"‚è±Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø—É—Å—Ç (–ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.2f}ms)")
            
            return memory_context
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏: {e}")
            return None

    async def _iter_processed_sentences(
        self,
        text: str,
        screenshot: Optional[str],
        memory_context: Optional[Dict[str, Any]],
        subscription_context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞."""
        import time
        enrich_start = time.time()
        enriched_text = self._enrich_context(text, memory_context, subscription_context)
        enrich_time = (time.time() - enrich_start) * 1000
        logger.info(f"‚è±Ô∏è  –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç—å—é –∑–∞–Ω—è–ª–æ {enrich_time:.2f}ms (–∏—Å—Ö–æ–¥–Ω—ã–π: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π: {len(enriched_text)} —Å–∏–º–≤–æ–ª–æ–≤)")

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64 (WebP)
        screenshot_data: Optional[str] = None
        if screenshot:
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64, –ø–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            screenshot_data = screenshot
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä (base64 –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 33% –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
            estimated_size = int(len(screenshot) * 0.75)
            logger.info(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç –ø–æ–ª—É—á–µ–Ω (WebP base64): ~{estimated_size} bytes (base64 –¥–ª–∏–Ω–∞: {len(screenshot)})")

        yielded_any = False
        llm_runtime_error: Optional[str] = None
        if self.text_module and hasattr(self.text_module, 'process'):
            llm_start = time.time()
            logger.info(f"‚è±Ô∏è  –ù–∞—á–∞–ª–æ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Text Module: '{enriched_text[:80]}...'")
            try:
                chunk_count = 0
                logger.info(f"üîÑ –í—ã–∑–æ–≤ _stream_text_module: text_len={len(enriched_text)}, has_screenshot={screenshot_data is not None}")
                async for chunk in self._stream_text_module(enriched_text, screenshot_data, session_id):
                    chunk_count += 1
                    logger.debug(f"üì¶ –ü–æ–ª—É—á–µ–Ω chunk #{chunk_count} –æ—Ç Text Module: type={type(chunk)}, value={str(chunk)[:100] if chunk else 'None'}...")
                    sentence = (self._extract_text_chunk(chunk) or '').strip()
                    if sentence:
                        if chunk_count == 1:
                            first_chunk_time = (time.time() - llm_start) * 1000
                            logger.info(f"‚è±Ô∏è  –ü–µ—Ä–≤—ã–π chunk –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {first_chunk_time:.2f}ms")
                        yielded_any = True
                        logger.info(f"üì® TextModule sentence #{chunk_count}: '{sentence[:120]}...' (len={len(sentence)})")
                        yield sentence
                    else:
                        logger.warning(f"‚ö†Ô∏è Chunk #{chunk_count} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è")
                llm_total_time = (time.time() - llm_start) * 1000
                logger.info(f"‚è±Ô∏è  LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {llm_total_time:.2f}ms: –ø–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks, yielded_any={yielded_any}")
                
                # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ LLM –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–∫—Å—Ç
                if not yielded_any:
                    logger.warning(
                        f"‚ö†Ô∏è LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: chunk_count={chunk_count}, enriched_text_len={len(enriched_text)}",
                        extra={
                            'scope': 'workflow',
                            'method': '_iter_processed_sentences',
                            'decision': 'warning',
                            'ctx': {
                                'reason': 'llm_empty',
                                'chunk_count': chunk_count,
                                'enriched_text_len': len(enriched_text)
                            }
                        }
                    )
            except Exception as processing_error:
                llm_runtime_error = str(processing_error)
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Text Module: {processing_error}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fail-open –æ—Ç–≤–µ—Ç")
                import traceback
                traceback.print_exc()
        elif self.text_module and hasattr(self.text_module, 'process_text_streaming'):
            # Legacy fallback –Ω–∞ –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ TextProcessor
            logger.info(f"üîÑ Legacy —Å—Ç—Ä–∏–º–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞: '{enriched_text[:80]}...'")
            try:
                json_buffer = ""  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON –∏–∑ —á–∞–Ω–∫–æ–≤
                json_attempts = 0  # –°—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
                MAX_JSON_BUFFER_SIZE = 10000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ (10KB)
                MAX_JSON_ATTEMPTS = 10  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON
                
                async for processed_sentence in self.text_module.process_text_streaming(enriched_text, screenshot_data, session_id=session_id):
                    # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ processed_sentence - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –∞ –Ω–µ —Ñ—É–Ω–∫—Ü–∏—è
                    if callable(processed_sentence):
                        logger.warning("‚ö†Ô∏è processed_sentence is callable, skipping")
                        continue
                    
                    sentence = (processed_sentence or '').strip()
                    if not sentence:
                        continue
                    
                    # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞
                    if len(json_buffer) + len(sentence) > MAX_JSON_BUFFER_SIZE:
                        logger.warning(f"‚ö†Ô∏è JSON –±—É—Ñ–µ—Ä –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç ({MAX_JSON_BUFFER_SIZE} —Å–∏–º–≤–æ–ª–æ–≤), —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                        yielded_any = True
                        yield json_buffer + sentence
                        json_buffer = ""
                        json_attempts = 0
                        continue
                    
                    # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON –∏–∑ —á–∞–Ω–∫–æ–≤
                    json_buffer += sentence
                    cleaned_buffer = self._extract_json_from_markdown(json_buffer)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –ø–æ–ª–Ω—ã–º JSON
                    if cleaned_buffer.strip().startswith('{'):
                        json_attempts += 1
                        try:
                            import json
                            parsed_json = json.loads(cleaned_buffer)
                            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–ª–Ω—ã–π JSON, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Ü–µ–ª–∏–∫–æ–º
                            if isinstance(parsed_json, dict):
                                logger.debug(f"‚úÖ Legacy: –ü–æ–ª–Ω—ã–π JSON –Ω–∞–∫–æ–ø–ª–µ–Ω: {len(json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤")
                                yielded_any = True
                                yield json.dumps(parsed_json, ensure_ascii=False)
                                json_buffer = ""
                                json_attempts = 0
                                continue
                        except (json.JSONDecodeError, ValueError):
                            # JSON –µ—â—ë –Ω–µ –ø–æ–ª–Ω—ã–π - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å
                            if json_attempts >= MAX_JSON_ATTEMPTS:
                                # –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
                                logger.warning(f"‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON ({MAX_JSON_ATTEMPTS}), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                                yielded_any = True
                                yield json_buffer
                                json_buffer = ""
                                json_attempts = 0
                                continue
                            logger.debug(f"üì¶ Legacy: –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ JSON: {len(json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {json_attempts}/{MAX_JSON_ATTEMPTS})")
                            continue
                    
                    # –ï—Å–ª–∏ –Ω–µ JSON –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω—ã–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                    if not json_buffer.strip().startswith('{'):
                        yielded_any = True
                        logger.debug(f"üì® Legacy TextProcessor sentence: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                        yield sentence
                        json_buffer = ""
                        json_attempts = 0
                
                # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞
                if json_buffer:
                    logger.debug(f"üì¶ Legacy: –û—Å—Ç–∞–ª—Å—è –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –±—É—Ñ–µ—Ä ({len(json_buffer)} —Å–∏–º–≤–æ–ª–æ–≤), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                    yielded_any = True
                    yield json_buffer
            except Exception as processing_error:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ legacy TextProcessor: {processing_error}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                import traceback
                traceback.print_exc()

        if not yielded_any:
            # –ï–¥–∏–Ω—ã–π fail-open –¥–ª—è runtime-–æ—à–∏–±–æ–∫ LLM:
            # –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º enriched prompt –æ–±—Ä–∞—Ç–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
            if llm_runtime_error:
                logger.warning(
                    "‚ö†Ô∏è LLM runtime error detected, emitting degraded response instead of prompt echo",
                    extra={
                        'scope': 'workflow',
                        'method': '_iter_processed_sentences',
                        'decision': 'degrade',
                        'ctx': {'error': llm_runtime_error[:300]}
                    }
                )
                yield "I can‚Äôt process this request right now because the AI provider is unavailable. Please try again in a minute."
                return

            logger.debug("‚ö†Ô∏è TextProcessor –Ω–µ –≤–µ—Ä–Ω—É–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–∞–∑–±–∏–≤–∫—É")
            for fallback_sentence in self._split_into_sentences(enriched_text):
                if fallback_sentence:
                    yield fallback_sentence

    async def _sanitize_for_tts(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return ""

        if self.text_filter_module and hasattr(self.text_filter_module, 'process'):
            try:
                result = await self.text_filter_module.process({
                    "operation": "clean_text",
                    "text": text,
                    "options": {
                        "remove_special_chars": True,
                        "remove_extra_whitespace": True,
                        "normalize_unicode": True,
                        "remove_control_chars": True
                    }
                })
                if isinstance(result, dict) and result.get("success") and result.get("cleaned_text") is not None:
                    return result.get("cleaned_text", "").strip()
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextFilterModule: %s", err)

        return text.strip()

    async def _split_complete_sentences(self, text: str) -> tuple[list[str], str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            logger.debug("‚ö†Ô∏è _split_complete_sentences: text –ø—É—Å—Ç–æ–π")
            return [], ""

        if self.text_filter_module and hasattr(self.text_filter_module, 'process'):
            try:
                result = await self.text_filter_module.process({
                    "operation": "split_sentences",
                    "text": text
                })
                if isinstance(result, dict) and result.get("success"):
                    sentences = result.get("sentences", [])
                    remainder = result.get("remainder", "")
                    logger.debug(f"‚úÖ TextFilterModule –≤–µ—Ä–Ω—É–ª: sentences={len(sentences)}, remainder_len={len(remainder)}")
                    return sentences, remainder
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextFilterModule: %s", err)

        # Fallback: –µ—Å–ª–∏ text_filter_module –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        stripped = text.strip()
        result = ([stripped] if stripped else [], "")
        logger.debug(f"üìù Fallback _split_complete_sentences: text_len={len(text)}, stripped_len={len(stripped)}, sentences={len(result[0])}")
        return result

    async def _count_meaningful_words(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á—ë—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return 0

        if self.text_filter_module and hasattr(self.text_filter_module, 'process'):
            try:
                result = await self.text_filter_module.process({
                    "operation": "count_meaningful_words",
                    "text": text
                })
                if isinstance(result, dict) and result.get("success"):
                    return int(result.get("count", 0))
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ TextFilterModule: %s", err)

        return len([w for w in text.split() if w.strip()])

    async def _stream_text_module(self, text: str, screenshot_data: Optional[str], session_id: Optional[str] = None):
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è."""
        logger.info(
            f"üîÑ _stream_text_module –≤—ã–∑–≤–∞–Ω: text_len={len(text)}, has_screenshot={screenshot_data is not None}",
            extra={
                'scope': 'workflow',
                'method': '_stream_text_module',
                'text_len': len(text),
                'has_screenshot': screenshot_data is not None
            }
        )
        
        payload: Dict[str, Any] = {"text": text}
        if screenshot_data:
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64 (WebP)
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64 (WebP)
            payload["image_data"] = screenshot_data
        
        if session_id:
            payload["session_id"] = session_id

        chunk_count = 0
        async for chunk in self._stream_module_results(self.text_module, payload, raise_errors=True):
            chunk_count += 1
            logger.debug(f"üì¶ _stream_text_module: –ø–æ–ª—É—á–µ–Ω chunk #{chunk_count}")
            yield chunk
        
        logger.info(
            f"‚úÖ _stream_text_module –∑–∞–≤–µ—Ä—à–µ–Ω: –ø–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks",
            extra={
                'scope': 'workflow',
                'method': '_stream_text_module',
                'chunk_count': chunk_count
            }
        )
        
        if chunk_count == 0:
            logger.warning(
                f"‚ö†Ô∏è _stream_text_module –Ω–µ –≤–µ—Ä–Ω—É–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ chunk",
                extra={
                    'scope': 'workflow',
                    'method': '_stream_text_module',
                    'decision': 'warning',
                    'ctx': {'reason': 'no_chunks_from_module', 'text_len': len(text)}
                }
            )

    async def _stream_audio_module(self, text: str):
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ –º–æ–¥—É–ª—è."""
        logger.info(
            f"üîÑ _stream_audio_module –≤—ã–∑–≤–∞–Ω: text_len={len(text)}",
            extra={
                'scope': 'workflow',
                'method': '_stream_audio_module',
                'text_len': len(text)
            }
        )
        
        chunk_count = 0
        total_bytes = 0
        async for chunk in self._stream_module_results(self.audio_module, {"text": text}):
            chunk_count += 1
            audio_bytes = self._extract_audio_chunk(chunk)
            if audio_bytes:
                total_bytes += len(audio_bytes)
            logger.debug(
                "üéµ _stream_audio_module: –ø–æ–ª—É—á–µ–Ω chunk #%s, bytes=%s",
                chunk_count,
                len(audio_bytes),
            )
            yield chunk
        
        logger.info(
            f"‚úÖ _stream_audio_module –∑–∞–≤–µ—Ä—à–µ–Ω: –ø–æ–ª—É—á–µ–Ω–æ {chunk_count} chunks, total_bytes={total_bytes}",
            extra={
                'scope': 'workflow',
                'method': '_stream_audio_module',
                'chunk_count': chunk_count,
                'total_bytes': total_bytes
            }
        )
        
        if chunk_count == 0:
            logger.warning(
                f"‚ö†Ô∏è _stream_audio_module –Ω–µ –≤–µ—Ä–Ω—É–ª –Ω–∏ –æ–¥–Ω–æ–≥–æ chunk",
                extra={
                    'scope': 'workflow',
                    'method': '_stream_audio_module',
                    'decision': 'warning',
                    'ctx': {'reason': 'no_audio_chunks_from_module', 'text_len': len(text)}
                }
            )

    async def _stream_module_results(self, module, payload: Dict[str, Any], raise_errors: bool = False):
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–∑–æ–≤ module.process —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π async generator."""
        if not module or not hasattr(module, 'process'):
            return
        try:
            result = await module.process(payload)
            if result is None:
                return
            if hasattr(result, "__aiter__"):
                async for item in result:
                    yield item
            else:
                yield result
        except Exception as err:
            logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –º–æ–¥—É–ª—è %s: %s", getattr(module, 'name', 'unknown'), err)
            if raise_errors:
                raise

    def _extract_text_chunk(self, chunk: Any) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥—É–ª—è.
        
        –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê LLM (—Å–æ–≥–ª–∞—Å–Ω–æ system prompt):
        - {"text": "...", "session_id": "..."}  ‚Äî text-only
        - {"text": "...", "session_id": "...", "command": "...", "args": {...}} ‚Äî action
        
        TextProcessingModule –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤: {'text': <llm_chunk>, 'type': 'text_chunk'}
        
        –í–ê–ñ–ù–û: –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ù–ò–ö–û–ì–î–ê –Ω–µ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è.
        –ï—Å–ª–∏ –Ω–µ —É–¥–∞—ë—Ç—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.
        """
        if chunk is None:
            return ""
        
        # –°–ª—É—á–∞–π 1: chunk —É–∂–µ —Å—Ç—Ä–æ–∫–∞
        if isinstance(chunk, str):
            chunk_stripped = chunk.strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ JSON-—Å—Ç—Ä–æ–∫–æ–π –æ—Ç LLM
            if chunk_stripped.startswith('{'):
                try:
                    import json
                    parsed = json.loads(chunk_stripped)
                    if isinstance(parsed, dict):
                        # –ï—Å–ª–∏ —ç—Ç–æ action-–æ—Ç–≤–µ—Ç —Å –∫–æ–º–∞–Ω–¥–æ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ü–û–õ–ù–´–ô JSON –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
                        if 'command' in parsed:
                            logger.debug(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω action-–æ—Ç–≤–µ—Ç: command={parsed.get('command')}")
                            return chunk_stripped
                        # –ï—Å–ª–∏ —ç—Ç–æ text-only –æ—Ç–≤–µ—Ç, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ text
                        if 'text' in parsed:
                            extracted = parsed['text']
                            if isinstance(extracted, str):
                                return extracted
                            # –ï—Å–ª–∏ text –Ω–µ —Å—Ç—Ä–æ–∫–∞, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                            logger.warning(f"‚ö†Ô∏è –ü–æ–ª–µ 'text' –Ω–µ —Å—Ç—Ä–æ–∫–∞: {type(extracted)}")
                            return str(extracted) if extracted else ""
                        # JSON –±–µ–∑ text –∏ –±–µ–∑ command ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (metadata)
                        logger.debug(f"‚ö†Ô∏è JSON –±–µ–∑ 'text' –∏ 'command', –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {list(parsed.keys())}")
                        return ""
                except (json.JSONDecodeError, ValueError):
                    # –ù–µ–ø–æ–ª–Ω—ã–π JSON ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≤ –±—É—Ñ–µ—Ä–µ
                    pass
            
            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return chunk
        
            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return chunk
        
        # –°–ª—É—á–∞–π 2: chunk ‚Äî —Å–ª–æ–≤–∞—Ä—å (–æ–±—ë—Ä—Ç–∫–∞ –æ—Ç TextProcessingModule)
        if isinstance(chunk, dict):
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: text -> text_response -> value -> chunk
            for key in ("text", "text_response", "value", "chunk"):
                value = chunk.get(key)
                if value is None:
                    continue
                
                # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî —Å—Ç—Ä–æ–∫–∞
                cleaned_value = str(value).strip()
                # –£–¥–∞–ª—è–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
                if cleaned_value.startswith("```"):
                     cleaned_value = cleaned_value.strip("`").replace("json", "").replace("python", "").strip()

                if isinstance(value, str):
                     # –ü—Ä–æ–≤–µ—Ä—è–µ–º JSON –≤ value
                     if cleaned_value.startswith('{') and "command" in cleaned_value:
                         return cleaned_value
                     return value
                
                # –ï—Å–ª–∏ text –Ω–µ —Å—Ç—Ä–æ–∫–∞, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                logger.warning(f"‚ö†Ô∏è –ü–æ–ª–µ '{key}' –Ω–µ —Å—Ç—Ä–æ–∫–∞: {type(value)}")
                return str(value)
            
            # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω—É–∂–Ω—ã—Ö –∫–ª—é—á–µ–π
            # –ü—Ä–æ–±—É–µ–º string representation —Å–ª–æ–≤–∞—Ä—è, –µ—Å–ª–∏ –æ–Ω –ø–æ—Ö–æ–∂ –Ω–∞ command
            chunk_str = str(chunk)
            if "'command':" in chunk_str or '"command":' in chunk_str:
                 logger.debug(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ª–æ–≤–∞—Ä—å-–∫–æ–º–∞–Ω–¥–∞: {chunk_str}")
                 try:
                     import json
                     return json.dumps(chunk)
                 except:
                     return chunk_str

            return ""

        return str(chunk)

        # –°–ª—É—á–∞–π 3: –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã
        # –ï—Å–ª–∏ —ç—Ç–æ —á—Ç–æ-—Ç–æ –∏–Ω–æ–µ ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–º–∏—Ç–∏–≤
        if isinstance(chunk, (int, float, bool)):
            return str(chunk)
        
        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø ‚Äî –ù–ï –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø chunk: {type(chunk)}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        return ""

    def _extract_audio_chunk(self, chunk: Any) -> bytes:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ –±–∞–π—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–æ–¥—É–ª—è."""
        if chunk is None:
            return b""
        if isinstance(chunk, (bytes, bytearray)):
            return bytes(chunk)
        if isinstance(chunk, dict):
            for key in ("audio", "audio_chunk", "data", "value"):
                value = chunk.get(key)
                if isinstance(value, (bytes, bytearray)):
                    return bytes(value)
        return b""

    def _enrich_context(
        self, 
        text: str, 
        memory_context: Optional[Dict[str, Any]],
        subscription_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ –ø–æ–¥–ø–∏—Å–∫–∏.
        –î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è LLM –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∫–æ–º–∞–Ω–¥.
        """
        context_parts = []
        
        # 1. Memory Context
        if memory_context:
            recent_memory = memory_context.get('recent_context', '')
            long_term_memory = memory_context.get('long_term_context', '')
            if recent_memory:
                context_parts.append(f"Memory Context (recent): {recent_memory}")
            if long_term_memory:
                context_parts.append(f"Memory Context (long-term): {long_term_memory}")
                
            # 2. Subscription Context & Instructions
        if subscription_context:
            status = subscription_context.get('status', 'unknown')
            sub_info = f"Subscription Status: {status}"
            if reason := subscription_context.get('reason'):
                sub_info += f" ({reason})"
            if limits := subscription_context.get('limits'):
                 sub_info += f"\nLimits: {limits}"
                 
            context_parts.append(sub_info)
            
        if not context_parts:
            return text
            
        prefix = "\n\n".join(context_parts)
        enriched_text = f"SYSTEM_CONTEXT:\n{prefix}\n\nUSER_INPUT:\n{text}"
        logger.debug(f"–¢–µ–∫—Å—Ç –æ–±–æ–≥–∞—â–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (len={len(enriched_text)})")
        return enriched_text

    async def _stream_audio_for_sentence(self, sentence: str, sentence_index: int) -> AsyncGenerator[bytes, None]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å—Ç—Ä–∏–º–∏—Ç —á–∞–Ω–∫–∏ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –∞—É–¥–∏–æ –ø–æ –º–µ—Ä–µ –∏—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency.
        
        Args:
            sentence: –¢–µ–∫—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ
            sentence_index: –ò–Ω–¥–µ–∫—Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            
        Yields:
            –ß–∞–Ω–∫–∏ –∞—É–¥–∏–æ (–ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)
        """
        if not sentence.strip():
            return
        if not self.audio_module:
            logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ")
            return
        
        # –°—Ç—Ä–∏–º–∏–º —á–∞–Ω–∫–∏ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è latency
        try:
            if hasattr(self.audio_module, 'process'):
                logger.debug(f"üîä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                chunk_count = 0
                async for chunk in self._stream_audio_module(sentence):
                    audio_chunk = self._extract_audio_chunk(chunk)
                    if audio_chunk:
                        chunk_count += 1
                        logger.debug(f"üîä Audio chunk #{chunk_count} –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(audio_chunk)} bytes")
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É, –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è
                        yield audio_chunk
                logger.debug(f"‚úÖ –ê—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {chunk_count} —á–∞–Ω–∫–æ–≤")
            elif hasattr(self.audio_module, 'generate_speech_streaming'):
                # Legacy fallback
                logger.debug(f"üîä Legacy –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(sentence)} —Å–∏–º–≤–æ–ª–æ–≤")
                chunk_count = 0
                async for audio_chunk in self.audio_module.generate_speech_streaming(sentence):
                    if audio_chunk:
                        chunk_count += 1
                        logger.debug(f"üîä Legacy audio chunk #{chunk_count} –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(audio_chunk)} bytes")
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫ —Å—Ä–∞–∑—É, –Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—è
                        yield audio_chunk
                logger.debug(f"‚úÖ Legacy –∞—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {chunk_count} —á–∞–Ω–∫–æ–≤")
                
        except Exception as audio_error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {audio_error}")
            raise
    
    async def _parse_assistant_response(self, response: Union[str, Dict[str, Any]], session_id: str):
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è text –∏ command_payload (–§–∞–∑–∞ 2)
        
        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å)
            session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            ParsedResponse —Å text_response –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º command_payload
        """
        try:
            config = get_config()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏—á–∞-—Ñ–ª–∞–≥ –∏ kill-switch
            if (not config.features.forward_assistant_actions or 
                config.kill_switches.disable_forward_assistant_actions):
                # –§–∏—á–∞ –≤—ã–∫–ª—é—á–µ–Ω–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                import json
                if isinstance(response, dict):
                    # FIXED: Use json.dumps to ensure valid JSON string (double quotes)
                    parsed = self._assistant_parser.parse(response.get('text', json.dumps(response, ensure_ascii=False)))
                else:
                    parsed = self._assistant_parser.parse(response)
                logger.info(
                    "[ACTION_PIPELINE][SERVER] stage=workflow_parse_bypassed session=%s has_command=%s text_len=%s",
                    session_id,
                    bool(parsed.command_payload),
                    len(parsed.text_response or ""),
                )
                return parsed
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç, –ø–µ—Ä–µ–¥–∞–≤–∞—è session_id –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ action-–æ—Ç–≤–µ—Ç—ã
            parsed = self._assistant_parser.parse(response, session_id=session_id)
            command_name = (
                parsed.command_payload.get('payload', {}).get('command')
                if parsed.command_payload else None
            )
            logger.info(
                "[ACTION_PIPELINE][SERVER] stage=workflow_parse session=%s has_command=%s command=%s text_len=%s",
                session_id,
                bool(parsed.command_payload),
                command_name,
                len(parsed.text_response or ""),
            )
            return parsed
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {e}, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
            import json
            if isinstance(response, dict):
                # FIXED: Use json.dumps to ensure valid JSON string (double quotes)
                text = response.get('text', json.dumps(response, ensure_ascii=False))
            else:
                text = str(response)
            parsed = self._assistant_parser.parse(text)
            logger.warning(
                "[ACTION_PIPELINE][SERVER] stage=workflow_parse_exception session=%s error=%s has_command=%s",
                session_id,
                e,
                bool(parsed.command_payload),
            )
            return parsed
    
    def _log_command_detected(self, parsed, session_id: str):
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã (–§–∞–∑–∞ 2)
        
        Args:
            parsed: ParsedResponse —Å command_payload
            session_id: ID —Å–µ—Å—Å–∏–∏
        """
        if not parsed.command_payload:
            return
        
        payload = parsed.command_payload.get('payload', {})
        command = payload.get('command', 'unknown')
        args = payload.get('args', {})
        
        log_structured(
            logger,
            logging.INFO,
            f"Command detected: {command}",
            scope="command",
            method="parse_assistant_response",
            decision="start",
            ctx={
                "session_id": session_id,
                "command": command,
                "args": args
            }
        )
    
    def _log_command_complete(self, command_payload: Optional[Dict[str, Any]], session_id: str):
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã (–§–∞–∑–∞ 2)
        
        Args:
            command_payload: Command payload –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∏–∑ ctx.pending_command_payload)
            session_id: ID —Å–µ—Å—Å–∏–∏
        """
        if not command_payload:
            return
        
        payload = command_payload.get('payload', {})
        command = payload.get('command', 'unknown')
        
        log_structured(
            logger,
            logging.INFO,
            f"Command forwarded: {command}",
            scope="command",
            method="process_request_streaming",
            decision="complete",
            ctx={
                "session_id": session_id,
                "command": command
            }
        )
    
    def _extract_json_from_markdown(self, text: str) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç Markdown-–æ–±—ë—Ä—Ç–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π JSON —Ç–µ–∫—Å—Ç.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ LLM:
        - ```json {...}```
        - ``` {...}```
        - json {...}
        - –¢–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON
        - –ß–∞—Å—Ç–∏—á–Ω—ã–π JSON (–¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è)
        - JSON —Å –ª–∏—à–Ω–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏/–ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏
        - JSON —Å trailing commas (—É–¥–∞–ª—è—é—Ç—Å—è)
        - JSON —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ (—É–¥–∞–ª—è—é—Ç—Å—è)
        
        Args:
            text: –¢–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å JSON –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
            
        Returns:
            –ß–∏—Å—Ç—ã–π JSON —Ç–µ–∫—Å—Ç –±–µ–∑ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        """
        if not text:
            return ""

        import re
        
        text = str(text).strip()

        # –í–∞—Ä–∏–∞–Ω—Ç 1: Markdown code fence ```json ... ``` –∏–ª–∏ ``` ... ```
        if text.startswith("```"):
            # –£–¥–∞–ª—è–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â–∏–π fence
            text = text[3:]
            text = text.lstrip()
            
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —è–∑—ã–∫ (json/JSON/JSONC –∏ —Ç.–¥.)
            lowered = text.lower()
            if lowered.startswith("json"):
                text = text[4:]
            text = text.lstrip()
            
            # –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫–∏
            while text.startswith(("\n", "\r")):
                text = text[1:]
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π fence (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –∫–æ–Ω—Ü–µ –∏–ª–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ JSON)
            text = text.rstrip()
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()

        # –í–∞—Ä–∏–∞–Ω—Ç 2: –¢–µ–∫—Å—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "json" (–±–µ–∑ markdown)
        # –£–¥–∞–ª—è–µ–º "json" –µ—Å–ª–∏ –æ–Ω —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ–¥ JSON –æ–±—ä–µ–∫—Ç–æ–º
        text_lower = text.lower()
        if text_lower.startswith("json") and len(text) > 4:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ "json" –∏–¥—ë—Ç –ø—Ä–æ–±–µ–ª/–ø–µ—Ä–µ–Ω–æ—Å –∏ –∑–∞—Ç–µ–º {
            after_json = text[4:].lstrip()
            if after_json.startswith("{") or after_json.startswith("\n{") or after_json.startswith("\r{"):
                text = after_json

        # –í–∞—Ä–∏–∞–Ω—Ç 3: –¢–µ–∫—Å—Ç –¥–æ/–ø–æ—Å–ª–µ JSON - –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ JSON –æ–±—ä–µ–∫—Ç
        # –ò—â–µ–º –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é
        first_brace = text.find("{")
        last_brace = text.rfind("}")
        
        if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –æ–±—ä–µ–∫—Ç
            json_candidate = text[first_brace:last_brace + 1]
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤–æ–∫—Ä—É–≥
            json_candidate = json_candidate.strip()
            
            # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã:
            # 1. –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (// –∏ /* */) - —Ö–æ—Ç—è JSON –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç, LLM –º–æ–∂–µ—Ç –∏—Ö –¥–æ–±–∞–≤–∏—Ç—å
            json_candidate = re.sub(r'//.*?$', '', json_candidate, flags=re.MULTILINE)  # –û–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            json_candidate = re.sub(r'/\*.*?\*/', '', json_candidate, flags=re.DOTALL)  # –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            
            # 2. –£–¥–∞–ª—è–µ–º trailing commas –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–º–∏ —Å–∫–æ–±–∫–∞–º–∏/—Ñ–∏–≥—É—Ä–Ω—ã–º–∏ —Å–∫–æ–±–∫–∞–º–∏
            json_candidate = re.sub(r',\s*}', '}', json_candidate)  # Trailing comma –ø–µ—Ä–µ–¥ }
            json_candidate = re.sub(r',\s*]', ']', json_candidate)  # Trailing comma –ø–µ—Ä–µ–¥ ]
            
            # 3. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            json_candidate = re.sub(r'\n\s*\n', '\n', json_candidate)  # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            json_candidate = re.sub(r'[ \t]+', ' ', json_candidate)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            
            # 4. –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –¥–≤–æ–µ—Ç–æ—á–∏–π –∏ –∑–∞–ø—è—Ç—ã—Ö
            json_candidate = re.sub(r'\s*:\s*', ': ', json_candidate)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ :
            json_candidate = re.sub(r'\s*,\s*', ', ', json_candidate)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ ,
            
            return json_candidate

        # –ï—Å–ª–∏ JSON –æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        # (–º–æ–∂–µ—Ç –±—ã—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π JSON –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è)
        return text.strip()

    def _split_into_sentences(self, text: str) -> list[str]:
        """
        –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
            import re
            sentences = re.split(r'[.!?]+', text)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            clean_sentences = [s.strip() for s in sentences if s.strip()]
            
            logger.debug(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(clean_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            return clean_sentences
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return [text]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            logger.info("–û—á–∏—Å—Ç–∫–∞ StreamingWorkflowIntegration...")
            self.is_initialized = False
            logger.info("‚úÖ StreamingWorkflowIntegration –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ StreamingWorkflowIntegration: {e}")
