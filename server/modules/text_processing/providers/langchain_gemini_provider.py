"""
LangChain Gemini Provider –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ Google Search

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangChain:
- –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (WebP/JPEG —á–µ—Ä–µ–∑ base64)
- Google Search
- –°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–æ–≤
"""

import logging
import base64
from typing import AsyncGenerator, Dict, Any, Optional, Union, TYPE_CHECKING
from integrations.core.universal_provider_interface import UniversalProviderInterface

if TYPE_CHECKING:
    from integrations.core.token_usage_tracker import TokenUsageTracker

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç—ã LangChain (—Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è)
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_core.messages import HumanMessage, SystemMessage
    LANGCHAIN_AVAILABLE = True
except ImportError:
    ChatGoogleGenerativeAI = None
    HumanMessage = None
    SystemMessage = None
    LANGCHAIN_AVAILABLE = False
    logger.warning("‚ö†Ô∏è LangChain –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø—Ä–æ–≤–∞–π–¥–µ—Ä –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")


def extract_text_from_chunk(chunk):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ chunk LangChain
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
    
    Args:
        chunk: Chunk –æ—Ç LangChain (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º, —Å–ø–∏—Å–∫–æ–º –∏–ª–∏ –æ–±—ä–µ–∫—Ç–æ–º)
        
    Returns:
        –°—Ç—Ä–æ–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ JSON –æ–±–µ—Ä—Ç–∫–∏).
        –ù–ò–ö–û–ì–î–ê –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è.
    """
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk.content –Ω–∞–ø—Ä—è–º—É—é (—Å—Ç–∞–Ω–¥–∞—Ä—Ç LangChain)
    if hasattr(chunk, 'content') and chunk.content:
        content = chunk.content
        # –ï—Å–ª–∏ content - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ (multimodal), –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if isinstance(content, list):
            texts = []
            for item in content:
                if isinstance(item, dict):
                    if 'text' in item:
                        texts.append(str(item['text']))
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º dict –±–µ–∑ 'text'
                elif isinstance(item, str):
                    texts.append(item)
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã
            return "".join(texts)
        if isinstance(content, str):
            return content
        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø content ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        return ""

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk.text (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if hasattr(chunk, 'text') and chunk.text:
        val = chunk.text
        if callable(val):
            return str(val())
        if isinstance(val, str):
            return val
        return ""
    
    # –ï—Å–ª–∏ chunk - —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if isinstance(chunk, list):
        texts = []
        for item in chunk:
            if isinstance(item, dict):
                if 'text' in item:
                    texts.append(str(item['text']))
                elif 'content' in item:
                    texts.append(str(item['content']))
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º dict –±–µ–∑ text/content
            elif isinstance(item, str):
                texts.append(item)
        return ''.join(texts)
    
    # –ï—Å–ª–∏ chunk - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
    if isinstance(chunk, dict):
        if 'text' in chunk:
            text_item = chunk['text']
            if isinstance(text_item, list):
                return ''.join([item.get('text', '') if isinstance(item, dict) else str(item) for item in text_item])
            if isinstance(text_item, str):
                # –ï—Å–ª–∏ —ç—Ç–æ JSON-–æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ workflow
                if text_item.strip().startswith('{'):
                    return text_item
                return text_item
            return ""
        elif 'content' in chunk:
            content = chunk['content']
            if isinstance(content, list):
                return ''.join([str(item) for item in content if isinstance(item, str)])
            if isinstance(content, str):
                return content
            return ""
        else:
            # Dict –±–µ–∑ text/content ‚Äî –ù–ï –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
            logger.debug(f"‚ö†Ô∏è LangChain chunk –±–µ–∑ text/content: {list(chunk.keys())}")
            return ""
    
    # –ï—Å–ª–∏ chunk - —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º content
    if hasattr(chunk, 'content'):
        content = chunk.content
        if isinstance(content, list):
            texts = []
            for item in content:
                if isinstance(item, dict):
                    if 'text' in item:
                        texts.append(str(item['text']))
                    elif 'type' in item and item.get('type') == 'text':
                        texts.append(str(item.get('text', '')))
                elif isinstance(item, str):
                    texts.append(item)
            return ''.join(texts)
        if isinstance(content, str):
            return content
        return ""
    
    # –ï—Å–ª–∏ chunk ‚Äî —Å—Ç—Ä–æ–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if isinstance(chunk, str):
        return chunk
    
    # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø ‚Äî –ù–ï –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
    logger.debug(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø LangChain chunk: {type(chunk)}")
    return ""


class LangChainGeminiProvider(UniversalProviderInterface):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangChain
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –ë–∞–∑–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞
    - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (WebP/JPEG —á–µ—Ä–µ–∑ base64, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é WebP)
    - Google Search
    - –°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    def __init__(self, config: Dict[str, Any], token_usage_tracker: Optional['TokenUsageTracker'] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LangChain Gemini –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            token_usage_tracker: –°–µ—Ä–≤–∏—Å —Ç—Ä–µ–∫–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        super().__init__(
            name="langchain_gemini",
            priority=1,  # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
            config=config
        )
        self.token_usage_tracker = token_usage_tracker
        
        self.model_name = config.get('model', 'gemini-3-flash-preview')
        self.temperature = config.get('temperature', 0.7)
        self.max_tokens = config.get('max_tokens', 2048)
        self.tools = config.get('tools', [])
        self.system_prompt = config.get('system_prompt', '')
        self.api_key = config.get('api_key', '')
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é WebP, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–∞–∫–∂–µ JPEG)
        self.image_mime_type = config.get('image_mime_type', 'image/webp')
        self.image_max_size = config.get('image_max_size', 10 * 1024 * 1024)
        
        # LangChain –∫–ª–∏–µ–Ω—Ç
        self.llm = None
        self.llm_with_tools = None
        self.llm_no_tools = None
        self.is_available = LANGCHAIN_AVAILABLE and bool(self.api_key)
        self.is_initialized = False
        
        logger.info(f"LangChainGeminiProvider initialized: available={self.is_available}")
    
    async def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LangChain
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            logger.info(f"üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê LangChainGeminiProvider.initialize():")
            logger.info(f"   ‚Üí is_available: {self.is_available}")
            logger.info(f"   ‚Üí api_key present: {bool(self.api_key)}")
            logger.info(f"   ‚Üí model_name: {self.model_name}")
            
            if not self.is_available:
                logger.error("Missing API key or LangChain dependencies")
                return False
            
            # –°–æ–∑–¥–∞–µ–º LangChain –∫–ª–∏–µ–Ω—Ç
            logger.info(f"üîç –°–æ–∑–¥–∞–µ–º LangChain –∫–ª–∏–µ–Ω—Ç...")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ tools –¥–ª—è Google Search
            model_kwargs = {}
            if self.tools and "google_search" in self.tools:
                model_kwargs["tools"] = [{"google_search": {}}]
                logger.info("‚úÖ Google Search –≤–∫–ª—é—á–µ–Ω")
            else:
                logger.info("‚ÑπÔ∏è  Google Search –≤—ã–∫–ª—é—á–µ–Ω (—Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –ø–æ–∏—Å–∫–∞)")

            # –°–æ–∑–¥–∞–µ–º LLM –±–µ–∑ tools (default)
            llm_params = {
                "model": self.model_name,
                "google_api_key": self.api_key,
                "temperature": self.temperature,
                "streaming": True,
            }

            if not ChatGoogleGenerativeAI:
                logger.error("ChatGoogleGenerativeAI –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω (LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)")
                return False

            self.llm_no_tools = ChatGoogleGenerativeAI(**llm_params)

            if model_kwargs:
                llm_with_tools_params = dict(llm_params)
                llm_with_tools_params["model_kwargs"] = model_kwargs
                self.llm_with_tools = ChatGoogleGenerativeAI(**llm_with_tools_params)
            else:
                self.llm_with_tools = None

            # Backward-compatible default
            self.llm = self.llm_with_tools or self.llm_no_tools

            logger.info("‚úÖ LangChain –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ LangChain...")
            test_query = "Hello"
            test_response = ""
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º: SystemMessage + HumanMessage
            messages = []
            if self.system_prompt and SystemMessage:
                messages.append(SystemMessage(content=self.system_prompt))
            if HumanMessage:
                messages.append(HumanMessage(content=test_query))
            
            async for chunk in self.llm.astream(messages):
                text = extract_text_from_chunk(chunk)
                if text:
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ text - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                    text_str = str(text) if not isinstance(text, str) else text
                    test_response += text_str
                    break  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ chunk –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
            if test_response:
                self.is_initialized = True
                logger.info(f"‚úÖ LangChain initialized: {self.model_name}")
                return True
            else:
                logger.error("‚ùå –¢–µ—Å—Ç–æ–≤–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ –æ—Ç–≤–µ—Ç")
                return False
                
        except Exception as e:
            logger.error(f"LangChain initialization failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def process(
        self,
        input_data: str,
        session_id: Optional[str] = None,
        use_search: Optional[bool] = None,
        system_prompt_override: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        –≠–¢–ê–ü 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LangChain
        –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
        
        Args:
            input_data: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Yields:
            –ß–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        """
        try:
            llm = self._select_llm(use_search)
            if not self.is_initialized or not llm:
                raise Exception("LangChain not initialized")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º: SystemMessage + HumanMessage
            # System prompt —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            messages = []
            system_prompt = system_prompt_override if system_prompt_override is not None else self.system_prompt
            if system_prompt and SystemMessage:
                messages.append(SystemMessage(content=system_prompt))
            if use_search is True and SystemMessage:
                messages.append(SystemMessage(content="You MUST use google_search for this request and base the answer on search results."))
            if HumanMessage:
                content = input_data
                if session_id:
                     content = f"{input_data}\n\n[System Context: session_id={session_id}]"
                messages.append(HumanMessage(content=content))
            
            # –°—Ç—Ä–∏–º–∏–Ω–≥ —á–µ—Ä–µ–∑ LangChain
            # –ù–ï —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–¥–µ—Å—å - —ç—Ç–æ –¥–µ–ª–∞–µ—Ç StreamingWorkflowIntegration
            
            # –î–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤
            accumulated_usage = None
            
            async for chunk in llm.astream(messages):
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å usage_metadata –∏–∑ —á–∞–Ω–∫–∞
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    accumulated_usage = chunk.usage_metadata
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk.text –Ω–∞–ø—Ä—è–º—É—é
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º extract_text_from_chunk –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                text = extract_text_from_chunk(chunk)
                if text:
                    yield text
                else:
                    # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º extract_text_from_chunk –µ—Å–ª–∏ text –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                    text = extract_text_from_chunk(chunk)
                    if text and text.strip():
                        yield text
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞
            if self.token_usage_tracker and accumulated_usage:
                try:
                    # Extract hardware_id from session or context if available
                    # Currently we don't have hardware_id passed explicitly to process,
                    # but maybe we can extract it or pass it.
                    # For now, we'll try to use session_id or 'unknown' if not available.
                    # Ideally, TextProcessor should pass hardware_id.
                    
                    # Note: We need hardware_id to record usage. 
                    # If session_id is UUID, we might be able to lookup hardware_id, 
                    # but simpler is to pass it. 
                    # For this step, we will use a placeholder or session_id 
                    # and rely on the calling layer to provide hardware_id if possible.
                    # Wait, usage table requires hardware_id.
                    
                    # We will assume session_id might be linked to hardware_id or used as fallback
                    # But hardware_id is NOT session_id.
                    # We need to update the signature of process to accept hardware_id or 
                    # make sure it's available.
                    
                    # Let's check update_token_usage_tracker logic. 
                    # We will use 'unknown' for now and fix it in TextProcessor
                    target_id = 'unknown' # Placeholder
                    
                    self.token_usage_tracker.record_usage(
                        hardware_id=target_id, 
                        source='main_llm',
                        input_tokens=accumulated_usage.get('input_tokens', 0),
                        output_tokens=accumulated_usage.get('output_tokens', 0),
                        session_id=session_id,
                        model_name=self.model_name
                    )
                except Exception as e:
                    logger.warning(f"Failed to record token usage: {e}")
                
            logger.debug("LangChain text processing completed")
                
        except Exception as e:
            logger.error(f"LangChain text processing error: {e}")
            raise e
    
    async def process_with_image(
        self,
        input_data: str,
        image_data: Union[str, bytes],
        session_id: Optional[str] = None,
        use_search: Optional[bool] = None,
        system_prompt_override: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        –≠–¢–ê–ü 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å WebP –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        
        Args:
            input_data: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            image_data: Base64 —Å—Ç—Ä–æ–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ WebP (–∏–ª–∏ bytes –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            
        Yields:
            –ß–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        """
        try:
            llm = self._select_llm(use_search)
            if not self.is_initialized or not llm:
                raise Exception("LangChain not initialized")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ image_data –Ω–µ None
            if image_data is None:
                logger.debug("No image data provided, processing as text only")
                async for chunk in self.process(input_data):
                    yield chunk
                return
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º image_data: –µ—Å–ª–∏ bytes - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64, –µ—Å–ª–∏ str - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if isinstance(image_data, bytes):
                # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∏ bytes, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
                image_b64 = base64.b64encode(image_data).decode('utf-8')
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
                estimated_size = len(image_data)
            elif isinstance(image_data, str):
                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64
                image_b64 = image_data
                # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä base64 —Å—Ç—Ä–æ–∫–∏ (base64 –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 33% –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
                estimated_size = int(len(image_b64) * 0.75)
            else:
                raise ValueError(f"image_data must be str (base64) or bytes, got {type(image_data)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            if estimated_size > self.image_max_size:
                raise ValueError(f"Image too large: ~{estimated_size} bytes (max {self.image_max_size})")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º content –¥–ª—è LangChain
            # –§–æ—Ä–º–∞—Ç: [{"type": "text", "text": "..."}, {"type": "image_url", "image_url": {"url": "data:{image_mime_type};base64,..."}}]
            # image_mime_type –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é image/webp, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å image/jpeg
            content = [
                {
                    "type": "text",
                    "text": f"{input_data}\n\n[System Context: session_id={session_id}]" if session_id else input_data
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{self.image_mime_type};base64,{image_b64}"
                    }
                }
            ]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º: SystemMessage + HumanMessage —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            messages = []
            system_prompt = system_prompt_override if system_prompt_override is not None else self.system_prompt
            if system_prompt and SystemMessage:
                messages.append(SystemMessage(content=system_prompt))
            if use_search is True and SystemMessage:
                messages.append(SystemMessage(content="You MUST use google_search for this request and base the answer on search results."))
            
            # HumanMessage —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            if HumanMessage:
                messages.append(HumanMessage(content=content))
            
            # –°—Ç—Ä–∏–º–∏–Ω–≥ —á–µ—Ä–µ–∑ LangChain
            # –ù–ï —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∑–¥–µ—Å—å - —ç—Ç–æ –¥–µ–ª–∞–µ—Ç StreamingWorkflowIntegration
            
            accumulated_usage = None
            
            async for chunk in llm.astream(messages):
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    accumulated_usage = chunk.usage_metadata
                    
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º chunk.text –Ω–∞–ø—Ä—è–º—É—é
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º extract_text_from_chunk –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
                text = extract_text_from_chunk(chunk)
                if text:
                    yield text
                else:
                    # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º extract_text_from_chunk –µ—Å–ª–∏ text –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                    text = extract_text_from_chunk(chunk)
                    if text and text.strip():
                        yield text
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            if self.token_usage_tracker and accumulated_usage:
                try:
                    target_id = 'unknown' # Placeholder
                    
                    self.token_usage_tracker.record_usage(
                        hardware_id=target_id,
                        source='main_llm',
                        input_tokens=accumulated_usage.get('input_tokens', 0),
                        output_tokens=accumulated_usage.get('output_tokens', 0),
                        session_id=session_id,
                        model_name=self.model_name
                    )
                except Exception as e:
                    logger.warning(f"Failed to record token usage (image): {e}")
                
            logger.debug("LangChain with image processing completed")
                
        except Exception as e:
            logger.error(f"LangChain with image processing error: {e}")
            raise e
    
    async def cleanup(self) -> bool:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        
        Returns:
            True –µ—Å–ª–∏ –æ—á–∏—Å—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            logger.info("Cleaning up LangChainGeminiProvider...")
            
            self.llm = None
            self.llm_with_tools = None
            self.llm_no_tools = None
            self.is_initialized = False
            
            logger.info("LangChainGeminiProvider cleaned up successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error cleaning up LangChainGeminiProvider: {e}")
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        """
        base_status = super().get_status()
        base_status.update({
            "model_name": self.model_name,
            "is_available": self.is_available,
            "has_google_search": "google_search" in self.tools if self.tools else False,
            "has_system_prompt": bool(self.system_prompt)
        })
        return base_status
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        """
        base_metrics = super().get_metrics()
        base_metrics.update({
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        })
        return base_metrics

    def _select_llm(self, use_search: Optional[bool]):
        if use_search is True and self.llm_with_tools:
            return self.llm_with_tools
        if use_search is False:
            return self.llm_no_tools
        return self.llm
